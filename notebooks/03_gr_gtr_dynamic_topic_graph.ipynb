{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gateway to Research - Topic Evolution\n",
    "\n",
    "In this notebook we measure the K factor of publication from ArXiv that have been tagged with _Field of Study_ (FoS) labels from Microsoft Academic Graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# install im_tutorial package\n",
    "!pip install git+https://github.com/nestauk/im_tutorials.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "\n",
    "# matplotlib for static plots\n",
    "import matplotlib.pyplot as plt\n",
    "# numpy for mathematical functions\n",
    "import numpy as np\n",
    "# pandas for handling tabular data\n",
    "import pandas as pd\n",
    "\n",
    "from im_tutorials.utilities import chunks\n",
    "from im_tutorials.data import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "from collections import Counter\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from collections import defaultdict\n",
    "from itertools import combinations, chain\n",
    "\n",
    "pd.set_option('max_columns', 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from sqlalchemy.engine.url import URL\n",
    "from sqlalchemy.engine import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from rhodonite.cooccurrence.basic import cooccurrence_graph\n",
    "from rhodonite.cooccurrence.cumulative import cumulative_cooccurrence_graph\n",
    "from rhodonite.cooccurrence.normalise import association_strength\n",
    "\n",
    "from gensim.models import LdaModel\n",
    "from annoy import AnnoyIndex\n",
    "from gensim.sklearn_api.ldamodel import LdaTransformer\n",
    "\n",
    "import graph_tool as gt\n",
    "from graph_tool.draw import graph_draw\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import networkx as nx\n",
    "import community\n",
    "\n",
    "from nesta.packages.nlp_utils.ngrammer import Ngrammer\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_projects = datasets.gateway_to_research_projects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw/gtr/gtr_projects.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to focus on data from just one funder, the EPSRC.\n",
    "\n",
    "After selecting EPSRC projects, we remove those with abstracts that have less than 200 characters. We then remove projects from before 2004 and after 2017 as there are few of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['funder_name'] == 'EPSRC']\n",
    "df = df[(df['start_year'] > 2004) & (df['start_year'] < 2018)]\n",
    "df = df.sort_values('start_year')\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(df['abstract_texts'].str.len(), bins=100)\n",
    "ax.set_title('Abstract Lengths')\n",
    "ax.set_xlabel('Number of Characters')\n",
    "ax.set_ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After seeing the abstract lengths, we will drop any that are very short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['abstract_texts'].str.len() >= 300]\n",
    "# df = df.drop_duplicates('abstract_texts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from im_tutorials.features.text_preprocessing import *\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenisation\n",
    "\n",
    "Typically, for computers to understand human language, it needs to be broken down in to components, e.g. sentences, syllables, or words.\n",
    "\n",
    "In the case of this work, we are going to analyse text at the word level. In natural language processing, the componenets below the sentence level are called **tokens**. The process of breaking a piece of text into tokens is called **tokenisation**. A token could be a word, number, email address or punctuation, depending on the exact tokenisation method used.\n",
    "\n",
    "For example, tokenising the  `'The dog chased the cat.'` might give `['The', 'dog', 'chased', 'the', 'cat', '.']`.\n",
    "\n",
    "In this case we will apply some extra processing during the tokenisation phase. We will\n",
    "\n",
    "1. Tokenise each document at the word level.\n",
    "2. Remove punctuation.\n",
    "3. Remove **stop words**, such as `the`, `and`, `to` etc.\n",
    "4. Apply lower case to all tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [list(chain(*tokenize_document(document))) for document in df['abstract_texts'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original text of first document:')\n",
    "print(df['abstract_texts'].values[0], '\\n')\n",
    "\n",
    "n_tokens_print = 10\n",
    "print(f'First {n_tokens_print} tokens in first document:')\n",
    "print(tokenized[0][:n_tokens_print])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [[wnl.lemmatize(t) for t in b] for b in tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N Grams\n",
    "\n",
    "We know that some in some cases, we might have words that appear together more often than we might expect. This might happen where we have commonly used phrases, or names of entities, for example `general relativity`. It can be useful to identify cases of this in our text so that the machine can understand that they represent different information when compared to the words appearing separately. Tokens of multiple words are called **n grams**. N grams containing two tokens are **bigrams**, n grams containing three words are **trigrams** and so on.\n",
    "\n",
    "For example, in a corpus of text, we might have the sentence, `'I travelled from York to New York to find a new life.'`. After tokenisation and finding bigrams, we might end up with `['i', 'travelled', 'from', 'york', 'to', 'new_york', 'to', 'find', 'a', 'new', 'life', '.']`.\n",
    "\n",
    "To create bigrams, we are going to use the natural language processing module **`gensim`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phraser, Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only find ngrams that appear 10 times or more\n",
    "phrases = Phrases(lemmas, min_count=10)\n",
    "phraser = Phraser(phrases)\n",
    "\n",
    "bigrammed = [phraser[t] for t in lemmas]\n",
    "\n",
    "\n",
    "print('Number of unique bigrams identified:', len(phraser.phrasegrams))\n",
    "print('Bigram examples:')\n",
    "print([b[0] for b in phraser.phrasegrams.items() if np.random.random() > 0.999], '\\n')\n",
    "n_tokens_print = 20\n",
    "print(f'First {n_tokens_print} tokens in first document after finding bigrams:')\n",
    "print(bigrammed[0][:n_tokens_print])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens_print = 20\n",
    "\n",
    "n_bigrams = len(phraser.phrasegrams)\n",
    "print('Number of unique bigrams identified:', len(phraser.phrasegrams), '\\n')\n",
    "print(f'{n_tokens_print} randomly selected bigrams:')\n",
    "print(['_'.join([s.decode() for s in b[0]]) \n",
    "       for b in phraser.phrasegrams.items() if np.random.random() > 1 - n_tokens_print*(1 / n_bigrams)], '\\n')\n",
    "\n",
    "print(f'First {n_tokens_print} tokens in first document after finding bigrams:')\n",
    "print(bigrammed[0][:n_tokens_print])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High Frequency Terms\n",
    "\n",
    "As well as stop words and punctuation, there may be other words that we want to remove, which are unique to our corpus. Often these are the tokens which appear very often and therefore convey little distinguishing information about each document.\n",
    "\n",
    "Let's count up all of the tokens in our processed corpus and see which are the most common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = Counter(chain(*bigrammed))\n",
    "token_counts.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to remove terms that occurr too often in our corpus. To do this, we need to pick a threshold value. A convenient way to do this is to use a max document frequency. In this case, we will say that if a token has appeared more times than a certain percentage of all the documents, it will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_doc_frequency = 0.4\n",
    "abstracts_tokenized = [[t for t in d if token_counts[t] < df.shape[0] * max_doc_frequency] for d in bigrammed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Human to Computer Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have preprocessed our text, we can apply various NLP techniques to further process, analyse, summarise the text, extract information from it, or use it as features in a later analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, when dealing with text, we need to somehow convert it in to numeric data that can be processed and analysed using mathematics. A very simple example would be to count the number of times each token appears in a document. For example if we have the sentence `'I like really cute cats, but all cats are cute really.'`, after pre-processing and tokenisation, we could generate a vector of word counts where each position represents the token count:\n",
    "\n",
    "```\n",
    "vector      token\n",
    "[1,         i\n",
    " 1,         like\n",
    " 2,         really\n",
    " 2,         cute\n",
    " 2,         cats\n",
    " 1,         but\n",
    " 1,         all\n",
    " 1,]        are\n",
    "```\n",
    "\n",
    "This method is called the **bag of words** approach, and in this case we can determine that the document is about really cute cats. But in real life, with many documents, things are not always so straightfoward.\n",
    "\n",
    "- What are some potential limitations of bag of words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(abstracts_tokenized)\n",
    "bow = [dictionary.doc2bow(d) for d in abstracts_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = 0\n",
    "print(f'Bag of words token frequencies for document {doc_id}:')\n",
    "print([(dictionary[b[0]], b[1]) for b in sorted(bow[doc_id], key=lambda x: x[1], reverse=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An improvement on the simple bag of words is to somehow weight each token by it's importance, or how much information it carries. One way to to do this is by weighting the count of each word in a document with the inverse of its frequency across _all_ documents. This is called **term frequency-inverse document frequency** or **tf-idf**.\n",
    "\n",
    "By doing this, a reasonably common word like `'height'` would probably be weighted lower than a less common, but more specific term such as `'altitude'`. Even if we have a document where height is mentioned more frequently than altitude, tf-idf can help us to identify that the document is referring to height in the context of altitude, rather than for example the height of a person.\n",
    "\n",
    "We will continue to use gensim for this functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfModel(bow, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = 0\n",
    "print([(dictionary[b[0]], '{:0.2f}'.format(b[1])) \n",
    "       for b in sorted(tfidf[bow[doc_id]], key=lambda x: x[1], reverse=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that terms that are much more specific are weighted relatively higher than those which convey higher level and more generic information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we have thousands of documents, which is too many for a single person to read and understand in a reasonable space of time. A useful first step is often to be able to understand what the main themes are within the documents we have. Bag of words or tf-idf are useful processing methods, but they still require us to inspect each document individually or group them and identify topics manually. \n",
    "\n",
    "Luckily, there are automated methods of finding the groups of tokens that describe broad themes within a set of documents, which are referred to as **topic modelling**.\n",
    "\n",
    "In this case, we are going to use **Latent Dirichlet Allocation** or **LDA**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.sklearn_api import LdaTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One aspect of many topic modelling methods, is that you have to specify the number of topics you expect in advance.\n",
    "\n",
    "- What are the disadvantages of this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 300\n",
    "\n",
    "lda = LdaModel(corpus=bow, id2word=dictionary, num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out some random topics and have a look at how coherent they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics_print = 10\n",
    "\n",
    "for topic_id in range(0, num_topics, int(num_topics/n_topics_print)):\n",
    "    print('Topic', topic_id)\n",
    "    print(lda.print_topic(topic_id), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_transformer = LdaTransformer(num_topics=num_topics, id2word=dictionary)\n",
    "lda_transformer.gensim_model = lda\n",
    "lda_vecs = lda_transformer.transform(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_topic_terms(topic_id, model, num_topics=None):\n",
    "#     num_topics = num_topics + 1\n",
    "#     topic_terms = [model.id2word[t[0]] for t \n",
    "#                    in model.get_topic_terms(topic_id)[:num_topics]]\n",
    "#     return topic_terms\n",
    "\n",
    "def make_topic_terms(model, num_topic_terms):\n",
    "    topic_terms = []\n",
    "    for i in range(model.num_topics):\n",
    "        topic_terms.append([model.id2word[t[0]] for t \n",
    "                   in model.get_topic_terms(i)[:num_topic_terms]])\n",
    "    return np.array(topic_terms)\n",
    "\n",
    "def make_topic_names(topic_vectors, topic_terms, num_topics=None):\n",
    "    topic_names = []\n",
    "    for vector in topic_vectors:\n",
    "        topic_ids = np.argsort(vector)[::-1][:num_topics]\n",
    "        name = ', '.join([c for c in chain(*topic_terms[topic_ids])])\n",
    "        topic_names.append(name)\n",
    "    return topic_names\n",
    "#     name = []\n",
    "#     for topic_id in topic_ids:\n",
    "#         topic_terms = get_topic_terms(topic_id, model, num_topic_terms)\n",
    "#         name.extend(topic_terms)\n",
    "#     return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_terms = make_topic_terms(lda, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = 0\n",
    "print(make_topic_names([lda_vecs[doc_id]], topic_terms, num_topics=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = make_topic_names(lda_vecs, topic_terms, num_topics=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualising Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 30\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "svd.fit(lda_vecs)\n",
    "svd_vecs = svd.transform(lda_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "tsne_vecs = tsne.fit_transform(svd_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 30\n",
    "kmm = KMeans(n_clusters=n_clusters)\n",
    "kms = kmm.fit_predict(svd_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.palettes import Plasma256\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = matplotlib.colors.Normalize(vmin=np.min(kms), vmax=np.max(kms))\n",
    "colors = [matplotlib.cm.colors.to_hex(cmap(norm(i))) for i in kms]\n",
    "cds = ColumnDataSource(data={'tsne_0': tsne_vecs[:, 0],\n",
    "                             'tsne_1': tsne_vecs[:, 1],\n",
    "                             'name': topic_names,\n",
    "                             'color': colors,\n",
    "                             'cluster': kms})\n",
    "\n",
    "p = figure(width=900)\n",
    "hover = HoverTool(tooltips=[(\"Topic\", \"@name\"), (\"Cluster\", \"@cluster\")])\n",
    "p.circle(source=cds, x='tsne_0', y='tsne_1', fill_color='color', line_color='color', \n",
    "         fill_alpha=0.5, line_alpha=0.5, radius=.5)\n",
    "p.add_tools(hover)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annoy_indices = {}\n",
    "for year, group in df.groupby(['start_year']):\n",
    "    ids = group.index.values\n",
    "\n",
    "    vecs = svd_vecs[ids]\n",
    "    t = AnnoyIndex(svd.n_components, 'angular')  # Length of item vector that will be indexed\n",
    "    for idx, vec in zip(ids, vecs):\n",
    "        t.add_item(idx, vec)\n",
    "    t.build(500)\n",
    "    annoy_indices[year] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = df['start_year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dist = 0.8\n",
    "\n",
    "project_edges = defaultdict(list)\n",
    "\n",
    "for year, group in df.groupby(['start_year']):\n",
    "    edges_year = []\n",
    "    ids = group.index.values\n",
    "    annoy_index = annoy_indices[year]\n",
    "    for idx in ids:\n",
    "        for neighbour_idx in annoy_index.get_nns_by_item(idx, 30):\n",
    "            if neighbour_idx == idx:\n",
    "                continue\n",
    "            else:\n",
    "                dist = annoy_index.get_distance(neighbour_idx, idx)\n",
    "                if dist < min_dist:\n",
    "                    edges_year.append((idx, neighbour_idx, {'dist': 1 - dist}))\n",
    "    project_edges[year].extend(edges_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_p = nx.Graph()\n",
    "g_p.add_edges_from(project_edges[2016])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_p_node_pos = nx.spring_layout(g_p, seed=101, weight='dist')\n",
    "nx.draw(g_p, pos=g_p_node_pos, node_size=15, node_color='C0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = community.best_partition(g_p, resolution=0.3, weight='dist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(g_p, pos=g_p_node_pos, node_size=15, node_color=list(communities.values()), cmap=matplotlib.cm.hsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 0.3\n",
    "\n",
    "project_communities = {}\n",
    "community_labels = {}\n",
    "for year, edge_list in project_edges.items():\n",
    "    g = nx.Graph()\n",
    "    g.add_edges_from(edge_list)\n",
    "    project_graphs[year] = g\n",
    "    \n",
    "    communities = community.best_partition(g, resolution=resolution, weight='dist')\n",
    "    print(f'N Communities at {year}:', len(set(communities.values())))\n",
    "    \n",
    "    community_ids = defaultdict(list)\n",
    "    for proj, c in communities.items():\n",
    "        community_ids[c].append(proj)\n",
    "    project_communities[year] = community_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_communities = {}\n",
    "\n",
    "for year, communities_year in project_communities.items():\n",
    "    lda_communities_year = []\n",
    "    for community_id, docs in communities_year.items():\n",
    "        mean_vec = np.mean(lda_vecs[docs], axis=0)\n",
    "        mean_vec = mean_vec / np.max(mean_vec)\n",
    "        lda_communities_year.append(mean_vec)\n",
    "    lda_communities[year] = lda_communities_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_communities = {}\n",
    "\n",
    "for year, communities_year in project_communities.items():\n",
    "    svd_communities_year = []\n",
    "    for community_id, docs in communities_year.items():\n",
    "        mean_vec = np.mean(svd_vecs[docs], axis=0)\n",
    "        mean_vec = mean_vec / np.max(mean_vec)\n",
    "        svd_communities_year.append(mean_vec)\n",
    "    svd_communities[year] = svd_communities_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbours = 3\n",
    "\n",
    "communities_edges = {}\n",
    "\n",
    "for year, vecs in lda_communities.items():\n",
    "    edges = []\n",
    "    for i, vec in enumerate(vecs):\n",
    "        similarities = [1 - cosine(vec, v) for v in vecs]\n",
    "        neighbours = np.argsort(similarities)[::-1][1:n_neighbours+1]\n",
    "        for n in neighbours:\n",
    "            edge = (i, n)\n",
    "            edges.append(edge)\n",
    "    communities_edges[year] = edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = nx.Graph()\n",
    "h.add_edges_from(communities_edges[2017])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = community.best_partition(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(h, node_color=list(communities.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_thresh = 0.8\n",
    "\n",
    "agg_edges = []\n",
    "max_parents = 1\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    if i > 0:\n",
    "        past_year = year - 1\n",
    "        past_vecs = svd_communities[past_year]\n",
    "        current_vecs = svd_communities[year]\n",
    "        for idx, vec in enumerate(current_vecs):\n",
    "            similarities = [1 - cosine(vec, c_past) for c_past in past_vecs]\n",
    "            sim_max_ids = np.argsort(similarities)[::-1][:max_parents]\n",
    "            for sim_max_idx in sim_max_ids:\n",
    "                edge = (f'{year}_{idx}', f'{past_year}_{sim_max_idx}', {'weight': similarities[sim_max_idx]})\n",
    "            agg_edges.append(edge)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = []\n",
    "for year, communities in project_communities.items():\n",
    "    for idx, _ in enumerate(communities):\n",
    "        nodes.append(f'{year}_{idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([e[2]['weight'] for e in agg_edges], bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = nx.DiGraph()\n",
    "h.add_nodes_from(nodes)\n",
    "h.add_edges_from(agg_edges)\n",
    "# h.add_edges_from([e for e in agg_edges if e[2]['weight'] > 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_x = np.array([int(d.split('_')[0]) for d in h.nodes])\n",
    "pos_x = pos_x - np.max(pos_x)\n",
    "\n",
    "tsne_agg = TSNE(n_components=1)\n",
    "svd_df = pd.DataFrame(np.array(list(chain(*svd_communities.values()))))\n",
    "pos = tsne_agg.fit_transform(svd_df)\n",
    "\n",
    "pos_y = pos_y - np.min(pos_y) \n",
    "pos_y = pos_y / np.max(pos_y)\n",
    "\n",
    "pos = {}\n",
    "for node, x, y in zip(h.nodes, pos_x, pos_y):\n",
    "    pos[node] = (x, y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_xy = TSNE(n_components=2)\n",
    "pos_xy = tsne_xy.fit_transform(list(chain(*svd_communities.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = int(np.round(np.mean([len(c) for c in svd_communities.values()])))\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=n_clusters)\n",
    "km.fit(list(chain(*svd_communities.values())))\n",
    "colors = km.labels_\n",
    "cmap_nodes = matplotlib.cm.hsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pos_xy[:, 0], pos_xy[:, 1], c=colors, cmap=cmap_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([1 / h.get_edge_data(e[0], e[1])['weight'] for e in h.edges])\n",
    "weights = weights / np.max(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "cmap = matplotlib.cm.get_cmap('inferno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 7.5))\n",
    "nx.draw(h, pos=pos, node_size=50, edge_color=weights, edge_cmap=cmap, width=2, node_color=colors, cmap=cmap_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_terms(topic_id, model, num_topics=None):\n",
    "    topic_terms = [lda.id2word[t[0]] for t in lda.get_topic_terms(topic_id)]\n",
    "    if num_topics is not None:\n",
    "        topic_terms = topic_terms[:num_topics+1]\n",
    "    return topic_terms\n",
    "    \n",
    "def make_topic_name(topic_vector, model, num_topics=None, num_topic_terms=None):\n",
    "    topic_ids = np.argsort(topic_vector)[::-1][:num_topics]\n",
    "    name = []\n",
    "    for topic_id in topic_ids:\n",
    "        topic_terms = get_topic_terms(topic_id, model, num_topic_terms)\n",
    "        name.extend(topic_terms)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, k in enumerate(lda_communities[2006]):\n",
    "    print('Community', i)\n",
    "    print(', '.join(make_topic_name(k, lda, num_topics=5, num_topic_terms=2)), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_topics = [', '.join(make_topic_name(t, lda, num_topics=5, num_topic_terms=1)) for t in chain(*svd_years)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_topic_attrs = {node_id: topic for node_id, topic in zip(h.nodes, node_topics)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_topic_attrs['2011_12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(h, node_topic_attrs, name='topic_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models.graphs import from_networkx\n",
    "from bokeh.models import HoverTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = figure(title=\"Networkx Integration Demonstration\", \n",
    "              x_range=(np.min(pos_x) - .5, np.max(pos_x) + .5), \n",
    "              y_range=(np.min(pos_y) - .1, np.max(pos_y) + .1),\n",
    "              width=900\n",
    "             )\n",
    "node_hover_tool = HoverTool(tooltips=[(\"Topic\", \"@topic_name\")])\n",
    "# Put everything on the plot.\n",
    "plot.add_tools(node_hover_tool)\n",
    "graph = from_networkx(h, pos, scale=2, center=(0,0))\n",
    "plot.renderers.append(graph)\n",
    "\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Co-Occurrence Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.max(lda_vecs, axis=1), bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(np.mean(lda_vecs, axis=1), 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rhodonite.cooccurrence.basic import cooccurrence_graph\n",
    "from rhodonite.cooccurrence.normalise import association_strength\n",
    "from graph_tool import GraphView\n",
    "from graph_tool.topology import label_largest_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_co_graphs = []\n",
    "\n",
    "for communities in community_labels:\n",
    "    main_topics = []\n",
    "    for c, ids in communities.items():\n",
    "        for vec in lda_vecs[ids]:\n",
    "            main_topics.append(np.nonzero(vec > 0)[0])\n",
    "    g, o_vprop, co_eprop = cooccurrence_graph(main_topics)\n",
    "    a_s = association_strength(g, o_vprop, co_eprop)\n",
    "    g.ep['a'] = a_s\n",
    "    vb, eb = betweenness(g)\n",
    "    g.vp['betweenness'] = vb\n",
    "    thresh = np.percentile(a_s.a, 90)\n",
    "    gv = GraphView(g, efilt=a_s.a > thresh)\n",
    "    topic_co_graphs.append(gv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_topics = []\n",
    "for vec in lda_vecs:\n",
    "    co_topics.append(np.nonzero(vec > 0)[0])\n",
    "g, o_vprop, co_eprop = cooccurrence_graph(co_topics)\n",
    "a_s = association_strength(g, o_vprop, co_eprop)\n",
    "g.ep['a'] = a_s\n",
    "thresh = np.percentile(a_s.a, 90)\n",
    "gv = GraphView(g, efilt=a_s.a > thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_tool.draw import fruchterman_reingold_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = fruchterman_reingold_layout(gv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gv in topic_co_graphs:\n",
    "    pos_this = gv.new_vertex_property('vector<double>')\n",
    "    pos_this.set_2d_array(pos.get_2d_array((0, 1)))\n",
    "    graph_draw(gv, vertex_fill_color=gv.vp['betweenness'], vcmap=matplotlib.cm.viridis, output_size=(200, 200),\n",
    "              pos=pos_this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.empty((len(years), lda.num_topics,))\n",
    "# a[:] = np.nan\n",
    "\n",
    "for i, (year, graph) in enumerate(zip(years, topic_co_graphs)):\n",
    "#     max_bs = np.argsort(graph.vp['betweenness'].a)[::-1][:10]\n",
    "    a[i, :] = graph.vp['betweenness'].a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_bs = np.argsort(np.max(a, axis=0))[::-1][:10]\n",
    "\n",
    "b = np.zeros((len(years), lda.num_topics,))\n",
    "# b[:] = np.nan\n",
    "\n",
    "for i, (year, graph) in enumerate(zip(years, topic_co_graphs)):\n",
    "    for m in max_bs:\n",
    "        b[i, m] = graph.vp['betweenness'][m]\n",
    "    b[i, :] = b[i, :] / np.max(b[i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_betweenness = pd.DataFrame(b)\n",
    "df_betweenness = df_betweenness[max_bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_betweenness.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure, ColumnDataSource\n",
    "from bokeh.palettes import Category10_10\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_betweenness.columns = [str(i) for i in df_betweenness.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cds = ColumnDataSource.from_df(df_betweenness.rolling(3).mean())\n",
    "p = figure(width=450, height=300)\n",
    "for c, color in zip(df_betweenness.columns, Category10_1010):\n",
    "    p.line(x='index', y=c, source=cds, color=color)\n",
    "    p.circle(x='index', y=c, source=cds, color=color)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = np.percentile(a_s.a, 50)\n",
    "gv = GraphView(g, efilt=a_s.a > thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d, c = eigenvector(gv)\n",
    "# c = closeness(gv)\n",
    "vb, eb = betweenness(gv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(vb.a, bins=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js = np.argsort(vb.a)[::-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in max_bs:\n",
    "    print(lda.print_topic(b), '\\n==========')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(a_s.a, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gv = GraphView(g, efilt=a_s.a > np.percentile(a_s.a, 95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = label_largest_component(gv)\n",
    "graph_draw(GraphView(gv, vfilt=l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_tool.centrality import eigenvector, closeness, betweenness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_draw(topic_co_graphs[0], vertex_fill_color=vb, vcmap=matplotlib.cm.viridis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvector??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev, ev_vprop = eigenvector(g, weight=a_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = closeness(g, weight=a_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(c.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_draw(GraphView(gv, vfilt=l), vertex_fill_color=ev_vprop, vcmap=matplotlib.cm.gist_heat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ideas**\n",
    "\n",
    "- Show changing topic centrality in topic coocurrence network\n",
    "- Average distance of each node to all nodes in previous year (plot as scatter)\n",
    "- Distance of each node to each other node in previous years (find which ones really are a combo and which ones are mono-topical\n",
    "- Which topics have been most stable over time? Which are not?\n",
    "- Calculate average centrality of communities over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "tsne_vecs = tsne.fit_transform(svd_vecs[list(g.nodes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = nx.draw(g, node_color=list(communities.values()), cmap='hsv', node_size=15, width=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_old.groupby('start_year')['project_id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = []\n",
    "\n",
    "for i, (year, group) in enumerate(df_old.groupby(['start_year'])):\n",
    "    ids = group.index.values\n",
    "    vertex_ids_map = {idx: vertex for vertex, idx in enumerate(ids)}\n",
    "    n_vertices = np.max(list(vertex_ids_map.values())) - 1\n",
    "    g = gt.Graph(directed=False)\n",
    "    g.add_vertex(n_vertices)\n",
    "    edge_list = edges[i]\n",
    "    edges_updated = []\n",
    "    for edge in edge_list:\n",
    "        edges_updated.append((vertex_ids_map[edge[0]], vertex_ids_map[edge[1]], edge[2]))\n",
    "    g.vp['doc_s'] = g.new_vertex_property('int')\n",
    "    g.vp['doc_t'] = g.new_vertex_property('int')\n",
    "    g.ep['dist'] = g.new_edge_property('float')\n",
    "    g.add_edge_list(edges_updated, eprops=[g.ep['dist']])\n",
    "    gt.stats.remove_parallel_edges(g)\n",
    "    graphs.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rhodonite.utils.tabular import edges_to_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_df = edges_to_dataframe(graphs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_draw(graphs[-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data at a Glance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_fos.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fos.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All ArXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = pd.to_datetime(df['created']).dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level 2 FoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_fos_2 = df_mag_fos[df_mag_fos['level'] == 2]\n",
    "df_fos_2 = df_fos.merge(df_mag_fos_2, left_on='fos_id', right_on='id', how='inner')\n",
    "article_fos_2 = df_fos_2.groupby('article_id')['name'].apply(list)\n",
    "article_fos_2 = article_fos_2.reset_index()\n",
    "article_fos_2['n_fos'] = [len(x) for x in article_fos_2['name']]\n",
    "article_fos_2 = article_fos_2[article_fos_2['n_fos'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df.merge(article_fos_2, left_on='id', right_on='article_id', how='inner')\n",
    "df_2 = df_2.sort_values('year')\n",
    "df_2 = df_2[df_2['year'] >= 1992]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(df_2['name'])\n",
    "df_2['fos_ids'] = [dictionary.doc2idx(d) for d in df_2['name']]\n",
    "df_2_years = df_2.groupby('year')['fos_ids'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g, o_props, o_cumsum_props, co_props, co_cumsum_props = cumulative_cooccurrence_graph(df_2_years.index.values, df_2_years.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rhodonite.utils.graph import subgraph_eprop_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph_eprop_values??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = 'Climate change'\n",
    "node = dictionary.token2id[term]\n",
    "vertices = [int(i) for i in g.vertex(3386).out_neighbours()]\n",
    "# vertices.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subgraph(g, vertices):\n",
    "    idx = g.new_vertex_property('bool')\n",
    "    idx.a[vertices] = True\n",
    "    gv = GraphView(g, vfilt=idx)\n",
    "    return gv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gv = get_subgraph(g, vertices)\n",
    "gv = GraphView(gv, efilt=co_props[2000].a > 0)\n",
    "l = label_largest_component(gv)\n",
    "gv = GraphView(gv, vfilt=l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_tool.inference.minimize import minimize_nested_blockmodel_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = minimize_nested_blockmodel_dl(gv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_draw(gv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level 0 FoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag_fos_0 = df_mag_fos[df_mag_fos['level'] == 0]\n",
    "df_fos_0 = df_fos.merge(df_mag_fos_0, left_on='fos_id', right_on='id', how='inner')\n",
    "article_fos_0 = df_fos_0.groupby('article_id')['name'].apply(list)\n",
    "article_fos_0 = article_fos_0.reset_index()\n",
    "article_fos_0['n_fos'] = [len(x) for x in article_fos_0['name']]\n",
    "article_fos_0 = article_fos_0[article_fos_0['n_fos'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = df.merge(article_fos_0, left_on='id', right_on='article_id', how='inner')\n",
    "df_0 = df_0.sort_values('year')\n",
    "df_0 = df_0[df_0['year'] >= 1992]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_0 = Dictionary(df_0['name'])\n",
    "df_0['fos_ids'] = [dictionary.doc2idx(d) for d in df_0['name']]\n",
    "df_0_years = df_0.groupby('year')['fos_ids'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0_years = df_0_years[df_0_years.index > 1998]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g0, o_props_0, o_cumsum_props_0, co_props_0, co_cumsum_props_0 = cumulative_cooccurrence_graph(df_0_years.index.values, df_0_years.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g, o_vprop, co_eprop = cooccurrence_graph(fos_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_s = association_strength(g, o_vprop, co_eprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_tool.draw import graph_draw\n",
    "from graph_tool.topology import label_largest_component\n",
    "from graph_tool import GraphView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = label_largest_component(g)\n",
    "gv = GraphView(g, vfilt=l, efilt=a_s.a > 10)\n",
    "l = label_largest_component(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_draw(gv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year_created'] = pd.to_datetime(df['created']).dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['year_created'] > 1991) & (df['year_created'] < 2019)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_fos = df_fos.groupby('article_id')['fos_id'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting HEP Articles and FoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year_created'] = pd.to_datetime(df['created']).dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['year_created'] > 1991) & (df['year_created'] < 2019)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_fos = df_fos.groupby('article_id')['fos_id'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get high energy physics category ids\n",
    "hep_cats = [a for a in df_cat['category_id'].value_counts().index if 'hep' in a]\n",
    "# get unique article ids for papers with hep categories\n",
    "hep_article_ids = df_cat.set_index('category_id').loc[hep_cats]['article_id'].unique()\n",
    "# get fields of study for hep papers\n",
    "hep_fos = df_fos.set_index('article_id').loc[hep_article_ids]\n",
    "hep_fos.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fos_id_2_level_map = {i: l for i, l in zip(df_mag_fos['id'].values, df_mag_fos['level'].values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(fos_id_2_level_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_fos_seqs = []\n",
    "for article_id, group in hep_fos.groupby('article_id'):\n",
    "    fos_ids = [f for f in group['fos_id'].values if not np.isnan(f)]\n",
    "    fos_ids = [f for f in fos_ids if  fos_id_2_level_map[f] > 2]\n",
    "    if len(fos_ids) > 2:\n",
    "        article_fos_seqs.append((article_id, fos_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fos(fos_list, level=2):\n",
    "    filtered = []\n",
    "    for fos in fos_list:\n",
    "        if np.isnan(fos):\n",
    "            continue\n",
    "        elif fos_id_2_level_map.loc[fos]['level'] > level:\n",
    "            continue\n",
    "        else:\n",
    "            filtered.append(fos)\n",
    "    return len(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hep_fos = pd.DataFrame(article_fos_seqs, columns=['article_id', 'fos_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hep = df.merge(df_hep_fos, left_on='id', right_on='article_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to years with enough data\n",
    "df_hep['created'] = pd.to_datetime(df_hep['created'])\n",
    "df_hep['year_created'] = df_hep['created'].dt.year\n",
    "df_hep = df_hep.sort_values('year_created')[(df_hep['year_created'] > 1991) & (df_hep['year_created'] < 2019)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get fos names for dictionary creation\n",
    "fos_id_2_name_mapping = {fos_id: name for fos_id, name in zip(df_mag_fos['id'].values, df_mag_fos['name'].values)}\n",
    "df_hep['fos_names'] = df_hep['fos_ids'].apply(lambda x: [fos_id_2_name_mapping[i] for i in x])\n",
    "# count number of fos for each paper\n",
    "df_hep['n_fos'] = [len(f) for f in df_hep['fos_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hep_n_fos_describe_df = df_hep.groupby('year_created')['n_fos'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hep_n_fos_describe_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hep.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rhodonite.cooccurrence.cumulative import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(df_hep['fos_names'])\n",
    "df_hep['fos_d_ids'] = [dictionary.doc2idx(d) for d in df_hep['fos_names']]\n",
    "# c = Counter(chain(*df_hep['fos_names']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = []\n",
    "for year, group in df_hep.groupby('year_created'):\n",
    "    communities.append(group['fos_d_ids'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = df_hep['year_created'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g, o_props, o_cumsum_props, co_props, co_cumsum_props = cumulative_cooccurrence_graph(\n",
    "    steps=steps, sequences=communities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I/O Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, co in co_graphs.items():\n",
    "    co.save('../data/processed/arxiv_co_graphs/arxiv_co_{}.gt'.format(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_tool import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dir = '../data/processed/arxiv_co_graphs'\n",
    "co_graphs = {}\n",
    "for file in sorted(os.listdir(graph_dir)):\n",
    "    year = file.split('.')[0][-4:]\n",
    "    g = Graph(directed=False)\n",
    "    g.load(os.path.join(graph_dir, file))\n",
    "    co_graphs[int(year)] = g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edges_2_dataframe(g):\n",
    "    edge_df = pd.DataFrame(list(g.edges()), columns=['s', 't'], dtype='int')\n",
    "    for k, ep in g.ep.items():\n",
    "        vt = ep.value_type()\n",
    "        if 'vector' not in vt:\n",
    "            if ('int' in vt) | ('bool' in vt):\n",
    "                edge_df[k] = ep.get_array()\n",
    "                edge_df[k] = edge_df[k].astype(int)\n",
    "            elif 'double' in vt:\n",
    "                edge_df[k] = ep.get_array()\n",
    "                edge_df[k] = edge_df[k].astype(float)\n",
    "    return edge_df\n",
    "\n",
    "def vertices_2_dataframe(g):\n",
    "    vertex_df = pd.DataFrame(list(g.vertices()), columns=['v'], dtype='int')\n",
    "    for k, vp in g.vp.items():\n",
    "        vt = vp.value_type()\n",
    "        if 'vector' not in vt:\n",
    "            if ('int' in vt) | ('bool' in vt):\n",
    "                vertex_df[k] = vp.get_array()\n",
    "                vertex_df[k] = vertex_df[k].astype(int)\n",
    "            elif 'double' in vt:\n",
    "                vertex_df[k] = vp.get_array()\n",
    "                vertex_df[k] = vertex_df[k].astype(float)\n",
    "    return vertex_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prop_dict_agg(prop_dict, aggfunc):\n",
    "    agg = []\n",
    "    for k, v in prop_dict.items():\n",
    "        agg.append(aggfunc(v.a))\n",
    "    return np.array(agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_tool import GraphView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric_prop(prop):\n",
    "    vt = prop.value_type()\n",
    "    if 'vector' in vt:\n",
    "        return False\n",
    "    elif 'bool' in vt:\n",
    "        return True\n",
    "    elif 'double' in vt:\n",
    "        return True\n",
    "    elif 'int' in vt:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def aggregate_edge_props(graph, aggfunc=np.mean):\n",
    "    results = {}\n",
    "    for prop_name, prop in graph.edge_properties.items():\n",
    "        if is_numeric_prop(prop):\n",
    "            results[prop_name] = aggfunc(prop.get_array())\n",
    "    results['num_edges'] = graph.num_edges()\n",
    "    return results\n",
    "\n",
    "def aggregate_vertex_props(graph, aggfunc=np.mean):\n",
    "    results = {}\n",
    "    for prop_name, prop in graph.vertex_properties.items():\n",
    "        if is_numeric_prop(prop):\n",
    "            results[prop_name] = aggfunc(prop.get_array())\n",
    "    results['num_nodes'] = graph.num_vertices()\n",
    "    return results\n",
    "\n",
    "def agg_props_to_df(graph_dict, prop_type='e', aggfunc=np.mean,\n",
    "                    label_name='year', edge_filter=None, vertex_filter=None):\n",
    "    records = []\n",
    "    for label, graph in graph_dict.items():\n",
    "        \n",
    "        if (edge_filter is not None) | (vertex_filter is not None):\n",
    "            if edge_filter is not None:\n",
    "                efilt = graph.ep[edge_filter]\n",
    "            else:\n",
    "                efilt = None\n",
    "            if vertex_filter is not None:\n",
    "                vfilt = graph.vp[vertex_filter]\n",
    "            else:\n",
    "                vfilt=None\n",
    "            gv = GraphView(graph, efilt=efilt, vfilt=vfilt)\n",
    "            gv = Graph(gv, prune=True)\n",
    "            if prop_type == 'e':\n",
    "                graph_agg_props = aggregate_edge_props(gv, aggfunc=aggfunc)\n",
    "            elif prop_type == 'v':\n",
    "                graph_agg_props = aggregate_vertex_props(gv, aggfunc=aggfunc)\n",
    "        else:\n",
    "            if prop_type == 'e':\n",
    "                graph_agg_props = aggregate_edge_props(graph, aggfunc=aggfunc)\n",
    "            elif prop_type == 'v':\n",
    "                graph_agg_props = aggregate_vertex_props(graph, aggfunc=aggfunc)\n",
    "        graph_agg_props[label_name] = label\n",
    "        records.append(graph_agg_props)\n",
    "        if (edge_filter is not None) | (vertex_filter is not None):\n",
    "            del gv\n",
    "            \n",
    "    return pd.DataFrame(records)\n",
    "        \n",
    "\n",
    "def edges_2_dataframe(graph):\n",
    "    '''edges_2_dataframe\n",
    "    Returns a dataframe with source and target columns.\n",
    "    '''\n",
    "    edge_df = pd.DataFrame(graph.get_edges(), columns=['s', 't', 'edge_index'])\n",
    "    return edge_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eprops_mean_df = agg_props_to_df(co_graphs)\n",
    "vprops_mean_df = agg_props_to_df(co_graphs, prop_type='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eprops_sum_df = agg_props_to_df(co_graphs, aggfunc=np.sum)\n",
    "vprops_sum_df = agg_props_to_df(co_graphs, aggfunc=np.sum, prop_type='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nodes and Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vertices = prop_dict_agg(o_props, np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(steps, n_vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 3.5))\n",
    "ax[0].plot(vprops_mean_df.set_index('year')['num_nodes'])\n",
    "ax[0].scatter(vprops_mean_df['year'].values, vprops_mean_df['num_nodes'].values)\n",
    "ax[0].set_xlabel('Year')\n",
    "ax[0].set_ylabel('N Nodes')\n",
    "ax[1].plot(eprops_mean_df.set_index('year')['num_edges'])\n",
    "ax[1].scatter(eprops_mean_df['year'].values, eprops_mean_df['num_edges'].values)\n",
    "ax[1].set_xlabel('Year')\n",
    "ax[1].set_ylabel('N Edges')\n",
    "ax[2].scatter(vprops_mean_df['num_nodes'], eprops_mean_df['num_edges'])\n",
    "ax[2].set_xlabel('N Nodes')\n",
    "ax[2].set_ylabel('N Edges')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/hep_n_nodes_edges.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a 26 year timespan of publications. Each year, the cumulative number of topics in our publications grows. This corresponds to the total number of nodes in our knowledge graph. There are two growth regimes in our data. Between 1992 and 2005 we see a fast rise in the number of topics, which begins to slow down. After 2005, we see linear growth. This means that we see a constant growth rate, despite the 'circle of knowledge' increasing in size. \n",
    "\n",
    "The number of edges between our topics grows linearly across all years. This only tells us about the number of new edges formed between a pair of nodes, and not how many existing edges were reinforced. \n",
    "\n",
    "In the earlier half of our time period, the rate of edge growth was slower compared to the number of nodes. After 2005, the rate speeds up, but the growth is linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Publications and Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('notebook', font_scale=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=3, figsize=(15, 3.5))\n",
    "axs[0].plot(hep_n_fos_describe_df.index, hep_n_fos_describe_df['count'])\n",
    "axs[0].scatter(hep_n_fos_describe_df.index, hep_n_fos_describe_df['count'])\n",
    "axs[0].set_xlabel('Year')\n",
    "axs[0].set_ylabel('N Publications')\n",
    "\n",
    "axs[1].scatter(hep_n_fos_describe_df['count'], vprops_mean_df['num_nodes'])\n",
    "axs[1].set_xlabel('N Publications')\n",
    "axs[1].set_ylabel('N Fields of Study')\n",
    "\n",
    "axs[2].plot(hep_n_fos_describe_df.index, hep_n_fos_describe_df['mean'].values)\n",
    "axs[2].scatter(hep_n_fos_describe_df.index, hep_n_fos_describe_df['mean'], label='Mean')\n",
    "axs[2].plot(hep_n_fos_describe_df.index, hep_n_fos_describe_df['25%'].values, color='C3')\n",
    "axs[2].scatter(hep_n_fos_describe_df.index, hep_n_fos_describe_df['25%'], color='C3', label='Lower 25%')\n",
    "axs[2].plot(hep_n_fos_describe_df.index, hep_n_fos_describe_df['75%'].values, color='C2')\n",
    "axs[2].scatter(hep_n_fos_describe_df.index, hep_n_fos_describe_df['75%'], color='C2', label='Upper 75%')\n",
    "axs[2].set_xlabel('Year')\n",
    "axs[2].set_ylabel('Fields of Study')\n",
    "axs[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/hep_n_papers.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The growth rate in the number of high energy physics papers published on ArXiv each year was highest at the start of the time span, and decreases over time towards a linear growth regime from around 2005. This mirrors the cumulative node count in our knowledge graph, as seen previously. The provenance of the sharp decline in 2018 is unknown.\n",
    "\n",
    "Indeed, the number of fields of study is proportional to the number of publications. In other words, the growth of topics discovered by our knowledge graph is proportional to the level of research effort in the system.\n",
    "\n",
    "The average number of fields of study per publication shows a very slight decline over time, from around 4.75 in 1992 to 4.5 in 2017. A first assumption might be that publications have become narrower in focus over time, however another interpretation might be that topics studied in more recent papers are not yet encapsulated by a term in the taxonomy, whereas older publications will cover more established concepts that are more likely to be included. As the methods used to create field of study labels in Microsoft Academic Graph are unknown, this is currently an unknown for our own methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Containment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def containment(a, b):\n",
    "    con = len(a.intersection(b))/ len(a)\n",
    "    return con\n",
    "\n",
    "cons = []\n",
    "i = 0\n",
    "for year, group in df_hep.groupby('year_created'):\n",
    "    c = Counter(chain(*group['fos_d_ids']))\n",
    "    s = set(c.keys())\n",
    "    if i > 0:\n",
    "        con = containment(s_old, s)\n",
    "        cons.append(con)\n",
    "    else:\n",
    "        cons.append(np.nan)\n",
    "    s_old = s.copy()\n",
    "    i += 1\n",
    "    \n",
    "hep_n_fos_describe_df['fos_containment'] = cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_cumsum = [np.nan]\n",
    "i = 0\n",
    "years = df_hep['year_created'].unique()\n",
    "for year in years[1:]:\n",
    "    s_old = set(chain(*df_hep[df_hep['year_created'] < year]['fos_d_ids']))\n",
    "    s_new = set(chain(*df_hep[df_hep['year_created'] == year]['fos_d_ids']))\n",
    "    cons_cumsum.append(containment(s_old, s_new))\n",
    "    \n",
    "hep_n_fos_describe_df['fos_cum_containment'] = cons_cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 3.5))\n",
    "ax[0].plot(hep_n_fos_describe_df.index, hep_n_fos_describe_df['fos_containment'])\n",
    "ax[0].scatter(hep_n_fos_describe_df.index, hep_n_fos_describe_df['fos_containment'])\n",
    "ax[0].set_xlabel('Year')\n",
    "ax[0].set_ylabel('Containment of Topics at $T_{-1}$')\n",
    "ax[0].set_ylim((0,1))\n",
    "\n",
    "ax[1].plot(hep_n_fos_describe_df.index, hep_n_fos_describe_df['fos_cum_containment'])\n",
    "ax[1].scatter(hep_n_fos_describe_df.index, hep_n_fos_describe_df['fos_cum_containment'])\n",
    "ax[1].set_xlabel('Year')\n",
    "ax[1].set_ylabel('Containment of Cumulative Topics')\n",
    "ax[1].set_ylim((0,1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/containment.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantity of new knowledge connections that can be made will be determined somewhat by the continued study of topics over time. The graph here shows the level to which topics at a given year contain the topics in studied in the previous year. We know that the number of overall number topics grows with each year, and now we can also see that the publications in each new time period consistently capture 70% of the topics from the preceding year.\n",
    "\n",
    "This tells us two things. First that there may be a consistent fraction of edges in one year that are a mix of either reinforcing edges or new edges between previously existing nodes. The other edges would be those formed between a pair of nodes that includes at least one new node. Second, due to the linear growth in the number of nodes, this fraction may be fairly constant, and provide an upper limit for the fraction of new edges that can be formed each year. In practice, this is unlikely to be reached as the number of possible edges far exceeds the number of publications, therefore we could think of it as contributing to a soft upper limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Share of Edges Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 3.5))\n",
    "\n",
    "ax[0].plot(eprops_mean_df.set_index('year')['new_co'][1:], label='New')\n",
    "ax[0].plot(eprops_mean_df.set_index('year')['reinforcing_co'][1:], label='Reinforcing')\n",
    "ax[0].plot(eprops_mean_df.set_index('year')['inactive_co'][1:], label='Inactive')\n",
    "ax[0].set_ylabel('% Edge Type')\n",
    "ax[0].set_xlabel('Year')\n",
    "ax[1].plot(eprops_sum_df.set_index('year')['new_co'][1:], label='New')\n",
    "ax[1].plot(eprops_sum_df.set_index('year')['reinforcing_co'][1:], label='Reinforcing')\n",
    "ax[1].plot(eprops_sum_df.set_index('year')['inactive_co'][1:], label='Inactive')\n",
    "ax[1].set_ylabel('Cooccurrence Count')\n",
    "ax[1].set_xlabel('Year')\n",
    "\n",
    "ax[2].plot(eprops_sum_df.set_index('year')['new_co'][1:], label='New')\n",
    "ax[2].plot(eprops_sum_df.set_index('year')['reinforcing_co'][1:], label='Reinforcing')\n",
    "ax[2].set_ylabel('Cooccurrence Count')\n",
    "ax[2].set_xlabel('Year')\n",
    "\n",
    "ax[0].set_ylim((0,1))\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[2].legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/share_of_edge_types.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we plot the number of new, reinforcing and inactive edges. Any edge must fall into one of these categories, and they are defined as follows:\n",
    "\n",
    "- **New**: an edge that connects two previously unconnected nodes.\n",
    "- **Reinforcing**: an edge that was formed in a previous period and is covered by at least one publication in the current period.\n",
    "- **Inactive**: an edge that was formed in a previous period and has not been covered by any publication in the current period.\n",
    "\n",
    "Over time, we can see that the number of inactive edges follows the same trend as the total number of edges in our knowledge graph. Inactive edges also dominate the graph at all years after the second year in our timespan, with their share growing from around 60% to over 90% between 1995 and 2018. In contrast, the proportion of new and reinforcing edges decreases as time goes on. As we might expect, reinforcing edges are more common than new edges, implying that a greater propotion of work done each year is at the intersection of two topics that were connected by previous researchers.\n",
    "\n",
    "What the proportions do not tell us is whether the overall number of new and reinforcing edges is actually growing over time. The absolute numbers show an interesting story. The number of inactive edges in each year marches steadily higher with a high rate of linear growth. Reinforcing edges also grow roughly linearly, though at a much lower volume and at a rate slower than inactive edges. However, new edges show no growth at all. The same number are created each year despite the fact that the numbers of publications and nodes both grow.\n",
    "\n",
    "Why is this? Perhaps because as we saw before, new topics become rarer with time, or perhaps another reason. Despite the growth in the number of topics, does it becomes harder for some reason to find completely new knowledge connections. Are the number of new edges driven by the number of new nodes or new connections between existing nodes? We should break the new edges down into further sub-categories to understand their dynamics.\n",
    "\n",
    "We can also further investigate the dynamics of reinforcing and inactive edges. Are reinforcing edges always in the same places? How old is the average inactive edge? How are the coccurrences that form new and reinforcing edges distributed? (i.e. are they concentrated on a small number of edges?) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering Edges by Node Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add another property to our nodes to signal whether they have been added that year or existed from a previous year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, co in co_graphs.items():\n",
    "    new_topic = co.new_vertex_property('bool')\n",
    "    co.vp['new_topic'] = new_topic\n",
    "    prev_year = year - 1\n",
    "    if prev_year in co_graphs:\n",
    "        prev_num_topics = co_graphs[prev_year].num_vertices()\n",
    "        for v in co.vertices():\n",
    "            if v >= prev_num_topics:\n",
    "                co.vp['new_topic'][v] = True\n",
    "            else:\n",
    "                co.vp['new_topic'][v] = False\n",
    "    else:\n",
    "        co.vp['new_topic'].a = [True for _ in range(co.num_vertices())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now lets add a new edge property that tells us whether our edges connect new nodes, existing nodes, or a combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, co in co_graphs.items():\n",
    "    \n",
    "    new_linkage = co.new_edge_property('bool')\n",
    "    mixed_linkage = co.new_edge_property('bool')\n",
    "    old_linkage = co.new_edge_property('bool')\n",
    "    \n",
    "    for e in co.edges():\n",
    "        s_new = co.vp['new_topic'][e.source()]\n",
    "        t_new = co.vp['new_topic'][e.target()]\n",
    "        new_edge = co.ep['new_co'][e]\n",
    "        if new_edge:\n",
    "            if s_new and t_new:\n",
    "                news += 1\n",
    "                new_linkage[e] = True\n",
    "                mixed_linkage[e] = False\n",
    "                old_linkage[e] = False\n",
    "            elif s_new and not t_new:\n",
    "                mixeds += 1\n",
    "                new_linkage[e] = False\n",
    "                mixed_linkage[e] = True\n",
    "                old_linkage[e] = False\n",
    "            elif t_new and not s_new:\n",
    "                mixeds += 1\n",
    "                new_linkage[e] = False\n",
    "                mixed_linkage[e] = True\n",
    "                old_linkage[e] = False\n",
    "            else:\n",
    "                olds += 1\n",
    "                new_linkage[e] = False\n",
    "                mixed_linkage[e] = False\n",
    "                old_linkage[e] = True\n",
    "        else:\n",
    "            oldens += 1\n",
    "            new_linkage[e] = False\n",
    "            mixed_linkage[e] = False\n",
    "            old_linkage[e] = False\n",
    "            \n",
    "    co.ep['new_linkage'] = new_linkage\n",
    "    co.ep['mixed_linkage'] = mixed_linkage\n",
    "    co.ep['old_linkage'] = old_linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eprops_new_linkage_mean_df = agg_props_to_df(co_graphs, edge_filter='new_linkage')\n",
    "eprops_mix_linkage_mean_df = agg_props_to_df(co_graphs, edge_filter='mixed_linkage')\n",
    "eprops_old_linkage_mean_df = agg_props_to_df(co_graphs, edge_filter='old_linkage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(eprops_new_linkage_mean_df['year'].values[1:], eprops_new_linkage_mean_df['num_edges'].values[1:],\n",
    "     label='New Topics')\n",
    "ax.plot(eprops_new_linkage_mean_df['year'].values[1:], eprops_mix_linkage_mean_df['num_edges'].values[1:],\n",
    "    label='Mixed Topics')\n",
    "ax.plot(eprops_new_linkage_mean_df['year'].values[1:], eprops_old_linkage_mean_df['num_edges'].values[1:],\n",
    "    label='Existing Topics')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('N Edges')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/new_edge_types.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the breakdown of new edges into 3 further sub-categories: those formed between two new topics, those formed between a new topic and an existing topic, and those formed between two existing topics that were not yet connected. The first two types show a slow decline in absolute numbers over time, although there are consistenly more mixed edges than ones between two new topics. New edges between existing topics stay at a relatively constant level throughought the timespan. The relative likelihoods between the three categories are in line with what we might expect; it is easier to create links that involve topics which already exist within the network. Another conclusion is that new topics are usually introduced to the knowledge graph by papers that combine them with a topic that is already known. New papers about topics that are all new are likely to be very rare. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this decline just a product of the fact that most topics are mentioned on a sub-annual basis? In this case, the number of new topics will be higher at periods near the start of the timespan even if they are active areas of study.\n",
    "\n",
    "We should investigate this and perhaps use age instead of the binary categorisation between new and not new."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vertex Age and Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, co in co_graphs.items():\n",
    "    age = co.new_vertex_property('int')\n",
    "    appearances = co.new_vertex_property('int')\n",
    "    frequency = co.new_vertex_property('double')\n",
    "    prev_year = year - 1\n",
    "    if prev_year in co_graphs:\n",
    "        co_prev = co_graphs[prev_year]\n",
    "    # age\n",
    "    num_prev_vertices = co_prev.num_vertices()\n",
    "    for v in co.vertices():\n",
    "        if co.vp['new_topic'][v]:\n",
    "            age[v] = 1\n",
    "            appearances[v] = 1\n",
    "        else:\n",
    "            age[v] = int(co_prev.vp['age'][int(v)]) + 1\n",
    "            if co.vp['o'][v] == 0:\n",
    "                appearances[v] = int(co_prev.vp['appearances'][int(v)])\n",
    "            else:\n",
    "                appearances[v] = int(co_prev.vp['appearances'][int(v)]) + 1\n",
    "\n",
    "    frequency.a = appearances.get_array() / age.get_array()\n",
    "    \n",
    "    co.vp['age'] = age\n",
    "    co.vp['appearances'] = appearances\n",
    "    co.vp['frequency'] = frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 3.5))\n",
    "ax[0].plot(vprops_mean_df['year'], vprops_mean_df['age'])\n",
    "ax[1].plot(vprops_mean_df['year'], vprops_mean_df['appearances'])\n",
    "ax[2].plot(vprops_mean_df['year'], vprops_mean_df['frequency'])\n",
    "ax[0].scatter(vprops_mean_df['year'], vprops_mean_df['age'])\n",
    "ax[1].scatter(vprops_mean_df['year'], vprops_mean_df['appearances'])\n",
    "ax[2].scatter(vprops_mean_df['year'], vprops_mean_df['frequency'])\n",
    "for a in ax:\n",
    "    a.set_xlabel('Year')\n",
    "ax[0].set_ylabel('Mean Topic Age')\n",
    "ax[1].set_ylabel('Mean Topic Appearances')\n",
    "ax[2].set_ylabel('Mean Topic Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define three new properties of a topic:\n",
    "- **Age**: the number of years since it was first introduced to the knowledge network\n",
    "- **Appearances**: the number of years that it has been mentioned in a publication\n",
    "- **Freqency**: appearances / age\n",
    "\n",
    "Here we plot the three properties. We can see that the mean age of topics increases linearly over time, rising by 0.67 years for each year in the timespan. This is due to the new nodes being added to the network.\n",
    "\n",
    "The number of appearances shows a different trend, tapering off slightly. This could mean that topics are revisted more infrequently over time.\n",
    "\n",
    "This appears to be the case from the third plot, which shows a continuing decrease in frequency over the timespan available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node and Edge DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also explore properties at the individual node and vertex level. This is important if we want to calculate a K Score at the publication level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_co_dfs = []\n",
    "e_co_dfs = []\n",
    "for year, co in co_graphs.items():\n",
    "    v_co_df = vertices_2_dataframe(co)\n",
    "    v_co_df['year'] = year\n",
    "    v_co_dfs.append(v_co_df)\n",
    "    \n",
    "    e_co_df = edges_2_dataframe(co)\n",
    "    e_co_df['year'] = year\n",
    "    e_co_dfs.append(e_co_df)\n",
    "    \n",
    "v_co_df = pd.concat(v_co_dfs)\n",
    "e_co_df = pd.concat(e_co_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_k_scores = []\n",
    "for year, group in df_hep.groupby('year_created'):\n",
    "    co = co_graphs[year]\n",
    "    for ids in group['fos_d_ids']:\n",
    "        k_scores = []\n",
    "        for combo in combinations(sorted(ids), 2):\n",
    "            if co.edge(combo[0], combo[1]) is not None:\n",
    "                k_scores.append(co.ep['k_score'][combo])\n",
    "        mean_k_scores.append(np.mean(k_scores))\n",
    "        \n",
    "df_hep['k_score'] = mean_k_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(df_hep.groupby('year_created')['k_score'].mean())\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Mean Publication K Score')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/mean_annual_k_score.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each publication, we query the edges of the graph at the relevant year, that correspond to the pairwise combinations of topics that it refers to. For each combination, we take the K Score and then take the average for all of them. Papers with many new edges will have a higher K Score than those with a high proportion of reinforcing edges. We can see that over time, the average score decreases and then settles around 0.3. Interestingly, this is at odds with the calculations from the WR Report data that shows a continuing decrease over a similar timespan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 3.5))\n",
    "sns.stripplot(df_hep['year_created'], df_hep['k_score'], color='C0', alpha=0.02, jitter=0.4, ax=ax)\n",
    "plt.setp(ax.get_xticklabels(), rotation=90)\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('K Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to plot the K Score for every publication over time, we can see that there are some common modes at 1, 0.5 and 0.33. This is likely due to the fact that there are so many edges which are visited only a handful of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 3.5))\n",
    "\n",
    "n_fos_agg = df_hep[df_hep['year_created'] > 2000]\n",
    "n_fos_agg_mean = n_fos_agg.groupby('n_fos')['k_score'].mean()\n",
    "ax[0].scatter(n_fos_agg_mean.index, n_fos_agg_mean.values)\n",
    "ax[0].set_xlabel('N Fields of Study')\n",
    "ax[0].set_ylabel('Mean K Score')\n",
    "\n",
    "ax[1] = sns.violinplot(n_fos_agg['n_fos'], n_fos_agg['k_score'], color='C0')\n",
    "ax[1].set_xlabel('N Fields of Study')\n",
    "ax[1].set_ylabel('K Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/n_fos_k_score.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we look at the trend of mean K Score by the number of fields of study contained within a paper. We do this publications published from 2001 and onwards, as this is the stable period wrt K Score during our timespan. We can see a relatively flat trend between publications that have only 3 FoS and those which have 8. \n",
    "\n",
    "Looking at the distributions in K Score for publications with different numbers of FoS, we can see a very interesting trend. As the number of fields of study increases, the proportion of publications with a very high and very low K Score decreases. This tells us that by covering more topics, the K score converges, however we're not sure whether this is because averaging a higher number of topics has a balancing effect, or whether publications that cover a larger number of topics are inherently less novel. What is also unclear is whether this is a real effect, or an artefact of the taxonomy and Microsoft Academic Graph's labelling algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To Do**\n",
    "- Is a new edge predictive of increased activity in that area above what we might expect among the population?\n",
    "- Or is an edge connecting a new node predictive of a higher publication activity in that area?\n",
    "- Does connecting to a new node predict that other nodes within the vicinity of the edge are more likely to connect next year?\n",
    "- Create edge property for growing edges (or can this already be deduced from existing edge props?)\n",
    "- Find node overlap each year\n",
    "- Understand the difference in the publication level K Score trend between ArXiv and WR data.\n",
    "- Look at the breakdown in K Score by edge for publications with different numbers of FoS. Does the breadth of topics covered by a publication actually impact the distribution of \n",
    "\n",
    "**Some caveats so far**\n",
    "- Decrease in average number of labels per paper could be that newer topics don't yet have a defined label.\n",
    "- Might need to normalise by number of papers in each year\n",
    "\n",
    "**Improvements**\n",
    "- Calculate age of edge using groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hep.to_csv('../data/processed/hep_arxiv_publications.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_co_df.to_csv('../data/processed/hep_co_vertices.csv', index=False)\n",
    "e_co_df.to_csv('../data/processed/hep_co_edges.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
