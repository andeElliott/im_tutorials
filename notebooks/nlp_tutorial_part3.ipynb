{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Tutorial\n",
    "\n",
    "### Elena Kochkina\n",
    "\n",
    "NESTA HackSTIR\n",
    "\n",
    "22.10.2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III. Topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "nltk.download('reuters')\n",
    "from nltk.corpus import reuters\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "documents_example = ['I like cats and dogs', 'Cats are furry animals', 'Dogs are good friends', 'Apples and carrots are healthy foods', 'Humans should maintain a healthy diet', 'If your diet consists of burgers it is not very healthy']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "bow_matrix = vectorizer.fit_transform(documents_example)\n",
    "vocabulary = vectorizer.get_feature_names()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation\n",
    "\n",
    "[documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "no_topics = 2\n",
    "lda = LatentDirichletAllocation(n_components=no_topics).fit(bow_matrix)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "no_top_words = 3\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "        print (\"Topic \", topic_idx)\n",
    "        print (\" \".join([vocabulary[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "  print (\"Topic \", topic_idx)\n",
    "  for i in range(len(vocabulary)):\n",
    "    print (vocabulary[i],topic[i])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "lda.transform(bow_matrix)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Reuters dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "documents_train = []\n",
    "labels_train = []\n",
    "documents_test = []\n",
    "labels_test = []\n",
    "#categories = reuters.categories()\n",
    "categories = ['wheat','gold','ship','coffee','grain']\n",
    "for cat in categories:\n",
    "  print (cat)\n",
    "  print (len(reuters.fileids(cat)))\n",
    "  for fileid in reuters.fileids(cat):\n",
    "    if fileid.startswith('training'):\n",
    "      documents_train.append(reuters.raw(fileid))\n",
    "      labels_train.append(cat)\n",
    "    else:\n",
    "      documents_test.append(reuters.raw(fileid))\n",
    "      labels_test.append(cat)\n",
    "  "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "documents_train_preprocessed = []\n",
    "for d in documents_train:\n",
    "  newd = d.lower()\n",
    "  newd = re.sub(r'[^A-Za-z0-9 ]+', '', newd)\n",
    "  documents_train_preprocessed.append(newd)\n",
    "  \n",
    "documents_test_preprocessed = []\n",
    "for d in documents_test:\n",
    "  newd = d.lower()\n",
    "  newd = re.sub(r'[^A-Za-z0-9 ]+', '', newd)\n",
    "  documents_test_preprocessed.append(newd)\n",
    "  \n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_train_bow = vectorizer.fit_transform(documents_train_preprocessed)\n",
    "X_test_bow = vectorizer.transform(documents_test_preprocessed)\n",
    "vocabulary = vectorizer.get_feature_names()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "no_topics = 5\n",
    "lda = LatentDirichletAllocation(n_components=no_topics).fit(X_train_bow)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "no_top_words = 5\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "        print (\"Topic \", topic_idx)\n",
    "        print (\" \".join([vocabulary[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "lda.transform(X_train_bow[0])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "lda.transform(X_test_bow[0])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
