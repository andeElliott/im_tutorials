{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Tutorial\n",
    "\n",
    "### Elena Kochkina\n",
    "\n",
    "NESTA HackSTIR\n",
    "\n",
    "22.10.2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('movie_reviews')\n",
    "from nltk.corpus import movie_reviews\n",
    "nltk.download('reuters')\n",
    "from nltk.corpus import reuters\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy\n",
    "import gensim \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "text_example = 'WHEN I found that I was a prisoner a sort of wild feeling came over me. I rushed up and down the stairs, trying every door and peering out of every window I could find; but after a little the conviction of my helplessness overpowered all other feelings. When I look back after a few hours I think I must have been mad for the time, for I behaved much as a rat does in a trap. When, however, the conviction had come to me that I was helpless I sat down quietly—as quietly as I have ever done anything in my life—and began to think over what was best to be done. I am thinking still, and as yet have come to no definite conclusion. Of one thing only am I certain; that it is no use making my ideas known to the Count. He knows well that I am imprisoned; and as he has done it himself, and has doubtless his own motives for it, he would only deceive me if I trusted him fully with the facts. So far as I can see, my only plan will be to keep my knowledge and my fears to myself, and my eyes open. I am, I know, either being deceived, like a baby, by my own fears, or else I am in desperate straits; and if the latter be so, I need, and shall need, all my brains to get through.'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*WHEN I found that I was a prisoner a sort of wild feeling came over me. I rushed up and down the stairs, trying every door and peering out of every window I could find; but after a little the conviction of my helplessness overpowered all other feelings. When I look back after a few hours I think I must have been mad for the time, for I behaved much as a rat does in a trap. When, however, the conviction had come to me that I was helpless I sat down quietly—as quietly as I have ever done anything in my life—and began to think over what was best to be done. I am thinking still, and as yet have come to no definite conclusion. Of one thing only am I certain; that it is no use making my ideas known to the Count. He knows well that I am imprisoned; and as he has done it himself, and has doubtless his own motives for it, he would only deceive me if I trusted him fully with the facts. So far as I can see, my only plan will be to keep my knowledge and my fears to myself, and my eyes open. I am, I know, either being deceived, like a baby, by my own fears, or else I am in desperate straits; and if the latter be so, I need, and shall need, all my brains to get through.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower casing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "text_example"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "lowercase_text_example = text_example.lower()\n",
    "lowercase_text_example"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of non-alphanumeric characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regular expressions** \n",
    "\n",
    "Sequences of characters that define a search pattern. \n",
    "\n",
    "\n",
    "[documentation](https://docs.python.org/3/howto/regex.html)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "alphanum_text_example = re.sub(r'[^A-Za-z0-9 ]+', '', lowercase_text_example)\n",
    "alphanum_text_example"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other regular expression examples include:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "content = \"Hi I am Elena, my website is https://warwick.ac.uk/ekochkina\"\n",
    "# remove URLs\n",
    "content = re.sub(r'\\b(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?\\b', '', content)\n",
    "# remove 3 characters and shorter\n",
    "content = re.sub(r'\\b[A-Za-z0-9]{1,3}\\b', '', content) \n",
    "content"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Process of splitting the text into chunks, called tokens. \n",
    "\n",
    "Often tokens==words, however tokens can be bigrams (word pairs), n-grams (sequences of n words), sentences, word chuncks or characters. \n",
    "\n",
    "[documentation](https://www.nltk.org/api/nltk.tokenize.html)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sent_tokenized_text = sent_tokenize(lowercase_text_example)\n",
    "print(sent_tokenized_text)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "word_tokenized_text = word_tokenize(alphanum_text_example)\n",
    "print(word_tokenized_text)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are alternative tokenization functions, e.g. there is a special tokenizer for tweets\n",
    "\n",
    "[spacy documentation](https://spacy.io/usage/spacy-101)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "word_tokenized_text = alphanum_text_example.split()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "spacy_doc = spacy_nlp(alphanum_text_example)\n",
    "for token in spacy_doc:\n",
    "    print(token.text)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop-word removal\n",
    "\n",
    "[nltk corpora documentation](https://www.nltk.org/book/ch02.html)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "stopwords.words(\"english\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "words_filtered = [w for w in word_tokenized_text if not w in stopwords.words(\"english\")]\n",
    "words_filtered"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alternative:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "words_filtered_spacy = []\n",
    "for token in spacy_doc:\n",
    "  if not token.is_stop:\n",
    "    words_filtered_spacy.append(token)\n",
    "words_filtered_spacy"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "words_filtered_spacy==words_filtered"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "\n",
    "[documentation](http://www.nltk.org/howto/stem.html)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ps = PorterStemmer()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "stemmed_words=[]\n",
    "for w in words_filtered:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "    \n",
    "stemmed_words  "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "lem = WordNetLemmatizer()\n",
    "\n",
    "word = \"flying\"\n",
    "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
    "print(\"Stemmed Word:\",ps.stem(word))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "word = \"acceptance\"\n",
    "print(\"Lemmatized Word:\",lem.lemmatize(word,\"n\"))\n",
    "print(\"Stemmed Word:\",ps.stem(word))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "lemmatized_words=[]\n",
    "for w in words_filtered:\n",
    "    lemmatized_words.append(lem.lemmatize(w,\"v\")) # need to know POS tag of each word\n",
    "    \n",
    "lemmatized_words"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alternative:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "words_lemmas_spacy = []\n",
    "for token in spacy_doc:\n",
    "  if not token.is_stop:\n",
    "    words_lemmas_spacy.append(token.lemma_)\n",
    "words_lemmas_spacy"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech (POS) tagging"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "nltk.pos_tag(words_filtered)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alternative:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "words_pos_spacy = []\n",
    "for token in spacy_doc:\n",
    "  if not token.is_stop:\n",
    "    words_pos_spacy.append(token.pos_) # for simple pos tag or use token.tag_ for detailed POS tag\n",
    "words_pos_spacy"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing and Named Entity Recognition\n",
    "\n",
    "Spacy has in built dependency parser and also Named Entity Recognition system"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for token in spacy_doc:\n",
    "  if not token.is_stop:\n",
    "    print (token.dep_)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for ent in spacy_doc.ents:\n",
    "  print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word frequency"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(words_lemmas_spacy)\n",
    "fdist.most_common(10)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sent_tokenized_text"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "len(sent_tokenized_text)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class sklearn.feature_extraction.text.CountVectorizer**(input=’content’, encoding=’utf-8’, decode_error=’strict’, strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=’(?u)\\b\\w\\w+\\b’, ngram_range=(1, 1), analyzer=’word’, max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class ‘numpy.int64’>)\n",
    "\n",
    "---\n",
    "Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "[documentation](https://https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "documents_example = sent_tokenized_text\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "bow_matrix = vectorizer.fit_transform(documents_example)\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "print(len(vocabulary))\n",
    "print(vocabulary)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bow_matrix.todense()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "numpy.shape(bow_matrix.todense())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_vectorizer.fit_transform(documents_example)\n",
    "bow_matrix = tfidf_vectorizer.fit_transform(documents_example)\n",
    "vocabulary = tfidf_vectorizer.get_feature_names()\n",
    "print(len(vocabulary))\n",
    "print(vocabulary)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bow_matrix.todense()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec embeddings\n",
    "\n",
    "\n",
    "[gensim documentation](https://radimrehurek.com/gensim/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained using large Google News corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download pre-trained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
    "!gzip -d GoogleNews-vectors-negative300.bin.gz"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!ls"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(model.similarity('man', 'woman'))\n",
    "print(model.similarity('king', 'queen'))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model.most_similar('king')[:5]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on your own corpus"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "movie_reviews.categories()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "movie_reviews.raw('neg/cv000_29416.txt')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "documents = []\n",
    "for fileid in movie_reviews.fileids():\n",
    "  documents.append(movie_reviews.raw(fileid))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def preprocess(text):\n",
    "  \n",
    "  lowercase = text.lower()\n",
    "  alphanum = re.sub(r'[^A-Za-z0-9 ]+', '', lowercase)\n",
    "  tokens = word_tokenize(alphanum)\n",
    "  \n",
    "  return tokens"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#preprocess(movie_reviews.raw('neg/cv000_29416.txt'))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "preprocessed_docs = []\n",
    "\n",
    "for d in documents:\n",
    "  preprocessed_docs.append(preprocess(d))\n",
    "  "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model_from_movie_reviews = gensim.models.Word2Vec(preprocessed_docs, min_count=1,size=50,workers=3, window=3, sg=1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(model_from_movie_reviews['king'])\n",
    "print(model_from_movie_reviews.similarity('queen', 'king'))\n",
    "print(model_from_movie_reviews.most_similar('king')[:5])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model_from_movie_reviews.save(\"word2vec_movie.model\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model_from_file = gensim.models.Word2Vec.load(\"word2vec_movie.model\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
