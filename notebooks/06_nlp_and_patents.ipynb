{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovering popular terminology within Patents\n",
    "\n",
    "This tutorial looks at the use of natural language processing to detect popular terminology within patents, and visualises the usage of such terminology over time.\n",
    "\n",
    "We will learn how to preprocess text data, transform words to numbers, convert the occurences to a time series and plot the timeseries.\n",
    "\n",
    "## What we will do:\n",
    "* Import the Python modules that will be used in the analysis.\n",
    "* Read the pre-prepared patent collections\n",
    "* Examine and discuss the data we have imported\n",
    "* Identify common terms with TF-IDF\n",
    "* Improve our results using stop words, frequency filtering and stemming\n",
    "* Identify popular terms through accumulation of TF-IDF scores\n",
    "* Convert TF-IDF scores to a time series of term occurance over time\n",
    "* Produce graphs of terms to investigate how usage changes over time\n",
    "\n",
    "\n",
    "## How is this tutorial structured:\n",
    "For every section, I will highlight its Goal and what we will do to achieve it. Then, I will explain the methods we use, what alternatives or additional thing we could do and lastly, we will run the code together. Note that some code cells can \"run\" for a while, so we will run them first and then explain what they do.\n",
    "\n",
    "## Download example patent data from PATSTAT\n",
    "\n",
    "We have already extracted a few sample datasets from the [PATSTAT](https://www.epo.org/searching-for-patents/business/patstat.html#tab-1) patents database.\n",
    "These are exported as Pandas DataFrames, so we just need to load them in.\n",
    "\n",
    "First of all, we need to prepare by loading in the support libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# install im_tutorial package\n",
    "!pip install git+https://github.com/nestauk/im_tutorials.git\n",
    "    \n",
    "# We also need S3 data support (to load our sample patents)\n",
    "!pip install smart_open\n",
    "\n",
    "# pandas - to manage data frames\n",
    "!pip install pandas\n",
    "\n",
    "# scikit-learn for our NLP pipeline\n",
    "!pip install scikit-learn\n",
    "\n",
    "# nltk for more NLP support (\"Natural Language ToolKit\")\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data\n",
    "\n",
    "Download the file from an S3 bucket... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from im_tutorials.data.ons import patents_10k, patents_100k\n",
    "\n",
    "df = patents_10k() \n",
    "# df = patents_100k() \n",
    "\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What have we acquired?\n",
    "Quickly check what data we've loaded... what attributes are available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example patent?\n",
    "What does the a random entry look like? Let's take a look at row 500..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.iloc[500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking for popular terminology\n",
    "\n",
    "We will use TF-IDF to find statistically popular terminology - where \"terminology\" is defined as a sequence of words. \n",
    "\n",
    "## TF-IDF\n",
    "\n",
    "TF-IDF is defined as \"Term Frequency - Inverse Document Frequency\", where the frequeny of a term in a document is divided by the number of documents it occurs in. This \"normalises\" a popular term by reducing its popularity by dividing by the number of documents it occurs in - if every document uses this term, it isn't very unusual, more likely to be a word such as \"the\" or \"and\".\n",
    "\n",
    "We use scikit-learn's implementation of TFIDF (refer to their [example of topic extraction](https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.htm) which uses TFIDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "from time import time\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(df.abstract)\n",
    "print(f\"Processed in {time() - t0:.2f}s.\")\n",
    "print(f'Number of features (words in our dictionary): {len(tfidf_vectorizer.get_feature_names()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfiltered results\n",
    "\n",
    "What words have we discovered? Let's look at the first 10 terms or \"feature names\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's a lot of 0's\n",
    "\n",
    "Oh dear. Maybe we should remove digits and punctuation? Let's just keep A-Z (assuming we are restricted to English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(token_pattern=r'[A-Za-z]+', analyzer='word')\n",
    "\n",
    "from time import time\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(df.abstract)\n",
    "print(f\"Processed in {time() - t0:.2f}s.\")\n",
    "print(f'Number of features (words in our dictionary): {len(tfidf_vectorizer.get_feature_names()):,}')\n",
    "tfidf_vectorizer.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just single words\n",
    "Looks better, but isolated words aren't very useful - no context. How about pairs or triplets of words? (bi-grams and tri-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(token_pattern=r'[A-Za-z]+', analyzer='word', ngram_range=(2,3))\n",
    "\n",
    "from time import time\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(df.abstract)\n",
    "print(f\"Processed in {time() - t0:.2f}s.\")\n",
    "print(f'Number of features (words in our dictionary): {len(tfidf_vectorizer.get_feature_names()):,}')\n",
    "tfidf_vectorizer.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-grams and tri-grams\n",
    "Yikes! That didn't help! Mind you \"a\" isn't a very useful word. Let's add in some \"stopwords\"..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(token_pattern=r'[A-Za-z]+', analyzer='word', \n",
    "                                   ngram_range=(2,3), stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(df.abstract)\n",
    "print(f\"Processed in {time() - t0:.2f}s.\")\n",
    "print(f'Number of features (words in our dictionary) after English stop words removed: {len(tfidf_vectorizer.get_feature_names()):,} bigrams and trigrams')\n",
    "tfidf_vectorizer.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unusual terms still present\n",
    "Hmmn. What if we skip rare terms, that could just be formatting or spelling errors? How about only terms that occur in at least 5 documents..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "minimum_document_frequency = 5\n",
    "tfidf_vectorizer = TfidfVectorizer(token_pattern=r'[A-Za-z]+', analyzer='word', \n",
    "                                   ngram_range=(2,3), stop_words='english', \n",
    "                                   min_df=minimum_document_frequency)\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(df.abstract)\n",
    "print(f\"Processed in {time() - t0:.2f}s.\")\n",
    "print(f'...after English stop words removed, remove terms occuring in less than {minimum_document_frequency} documents:'\n",
    "      f' {len(tfidf_vectorizer.get_feature_names()):,} bigrams and trigrams')\n",
    "tfidf_vectorizer.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meaningful bi- and tri-grams\n",
    "That's better! That's really reduced the number of n-grams. What else have we got?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer.get_feature_names()[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same words, different forms?\n",
    "Hmmn. That's a lot of variants of 'absorb'. If we had a \"stemmer\" we could remove common endings to get to the common \"stem\" (note that this is different to lemmatising - lemmas are the basic form of the word, but require a dictionary - patent words might not all be in the dictionary).\n",
    "\n",
    "First of all, let's load NLTK's library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A \"stemming\" tokenizer\n",
    "\n",
    "We need a piece of code that can extract words (\"tokens\") from a stream of text - and \"stem\" the words..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "class StemTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.ps = nltk.PorterStemmer()\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        return [self.ps.stem(t) for t in word_tokenize(doc)]\n",
    "\n",
    "t = StemTokenizer()\n",
    "t('absorbs absorbing absorber absorption 123')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming Tokenizer ready\n",
    "\n",
    "Looks good, multiple forms of \"absorb\" are now mapped to a single stem - shame about \"absorption\" - a lemmatiser could map this to \"absorb\" if it was in the lemmatiser's dictionary.\n",
    "\n",
    "Never mind, let's try it with the patent abstracts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "minimum_document_frequency = 5\n",
    "tfidf_vectorizer = TfidfVectorizer(token_pattern=r'[A-Za-z]+', analyzer='word', \n",
    "                                   ngram_range=(2,3), stop_words='english', \n",
    "                                   min_df=minimum_document_frequency,\n",
    "                                   tokenizer=StemTokenizer())\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(df.abstract)\n",
    "print(f\"Processed in {time() - t0:.2f}s.\")\n",
    "print(f'...after English stop words removed, remove terms occuring in less than {minimum_document_frequency} documents:'\n",
    "      f' {len(tfidf_vectorizer.get_feature_names()):,} bigrams and trigrams')\n",
    "tfidf_vectorizer.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What went wrong?\n",
    "\n",
    "Oh dear. Tokenizer overrides the regular expression, so we'll have to combine the two..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "class StemTokenizerWithWordFilter(object):\n",
    "    def __init__(self):\n",
    "        self.ps = nltk.PorterStemmer()\n",
    "        self.token_pattern = re.compile(r'[A-Za-z]+')\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        return [self.ps.stem(t) for t in self.token_pattern.findall(doc)]\n",
    "\n",
    "t = StemTokenizerWithWordFilter()\n",
    "t('absorbs absorbing absorber absorption 123')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemmer revisited\n",
    "\n",
    "Great - digits are removed, and the \"absorb\" stemming still works - so let's try again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "minimum_document_frequency = 5\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', \n",
    "                                   ngram_range=(2,3), stop_words='english',\n",
    "                                   min_df=minimum_document_frequency,\n",
    "                                   tokenizer=StemTokenizerWithWordFilter())\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(df.abstract)\n",
    "print(f\"Processed in {time() - t0:.2f}s.\")\n",
    "print(f'...after English stop words removed, remove terms occuring in less than {minimum_document_frequency} documents:'\n",
    "      f' {len(tfidf_vectorizer.get_feature_names()):,} bigrams and trigrams')\n",
    "tfidf_vectorizer.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errors from scikit-learn?\n",
    "\n",
    "Ah - yes, we are comparing stemmed words with the original stopword list which isn't stemmed. Whoops. Let's stem the stopwords so they will match the output of the stemmer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words_as_string = \" \".join(stop_words)\n",
    "stemmed_stop_words = StemTokenizerWithWordFilter()(stop_words_as_string)\n",
    "stemmed_stop_words_no_duplicates = list(set(stemmed_stop_words))\n",
    "stemmed_stop_words_no_duplicates[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemmed stopwords\n",
    "Let's analyse the patents again, this time with the stopwords matching the output of a our stemmer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "minimum_document_frequency = 5\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', \n",
    "                                   ngram_range=(2,3), \n",
    "                                   stop_words=stemmed_stop_words_no_duplicates,\n",
    "                                   min_df=minimum_document_frequency,\n",
    "                                   tokenizer=StemTokenizerWithWordFilter())\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(df.abstract)\n",
    "print(f\"Processed in {time() - t0:.2f}s.\")\n",
    "print(f'...after English stop words removed, remove terms occuring in less than {minimum_document_frequency} documents:'\n",
    "      f' {len(tfidf_vectorizer.get_feature_names()):,} bigrams and trigrams')\n",
    "tfidf_vectorizer.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeated words?\n",
    "\n",
    "Hmmn. Slightly odd - \"absorb heat heat\" etc.; let's see what else we have... let's look at the following 10 terms..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer.get_feature_names()[10:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, not so bad after all. Hopefully we've now got a sensible feature set - what features are of interest?\n",
    "\n",
    "# Features of interest\n",
    "\n",
    "One approach is to look at the TFIDF matrix; each row represents a document, each column a feature (i.e. an \"n-gram\"). A feature is of interest if it is popular and interesting - by that we mean it appears repeatedly in a document but not in all documents. Or, in other words, a high TF-IDF value against a term.\n",
    "\n",
    "Let's try collapsing the matrix by summing the rows; this will reveal which features have the highest weights and in turn which n-grams are of interest..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "summed_tfidf = tfidf.sum(axis=0)\n",
    "summed_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which term accumulated what TF-IDF total?\n",
    "Let's associate the n-grams with their scores..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "summed_tfidf_list = summed_tfidf.tolist()[0]\n",
    "print(len(summed_tfidf_list))\n",
    "\n",
    "ngram_list = tfidf_vectorizer.get_feature_names()\n",
    "print(len(ngram_list))\n",
    "\n",
    "ngram_scores = list(zip(summed_tfidf_list, tfidf_vectorizer.get_feature_names()))\n",
    "ngram_scores[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which terms have the highest accumulated TF-iDF score?\n",
    "So if we sort the tuples by TF-IDF accumulated score..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sorted_ngram_scores = sorted(ngram_scores, key=lambda tup: tup[0], reverse=True)\n",
    "sorted_ngram_scores[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have popular terminology! Is it meaningful?\n",
    "\n",
    "Now we're getting somewhere! However, there are a number of n-grams that aren't useful:\n",
    "* util model (\"utility model\"?)\n",
    "* least one (\"...at least one...\"?)\n",
    "* invent relat (\"invention related\"?)\n",
    "* present invent (\"present invention\"?)\n",
    "* invent disclos (\"invention disclosed\"?)\n",
    "\n",
    "Suggest we add \"invention\" to the stopword list...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stemmed_stop_words_custom = stemmed_stop_words_no_duplicates + ['invent', 'util', 'disclos', 'problem', 'solv', 'becau', 'copyright', 'one']\n",
    "stemmed_stop_words_custom[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerun with revised stopwords\n",
    "Let's try again, with the revised list of words to ignore..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "minimum_document_frequency = 5\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', \n",
    "                                   ngram_range=(2,3), \n",
    "                                   stop_words=stemmed_stop_words_custom,\n",
    "                                   min_df=minimum_document_frequency,\n",
    "                                   tokenizer=StemTokenizerWithWordFilter())\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(df.abstract)\n",
    "print(f\"Processed in {time() - t0:.2f}s.\")\n",
    "print(f'...after English stop words removed, remove terms occuring in less than {minimum_document_frequency} documents:'\n",
    "      f' {len(tfidf_vectorizer.get_feature_names()):,} bigrams and trigrams')\n",
    "\n",
    "summed_tfidf = tfidf.sum(axis=0)\n",
    "summed_tfidf_list = summed_tfidf.tolist()[0]\n",
    "print(len(summed_tfidf_list))\n",
    "\n",
    "ngram_list = tfidf_vectorizer.get_feature_names()\n",
    "print(len(ngram_list))\n",
    "\n",
    "ngram_scores = list(zip(summed_tfidf_list, tfidf_vectorizer.get_feature_names()))\n",
    "sorted_ngram_scores = sorted(ngram_scores, key=lambda tup: tup[0], reverse=True)\n",
    "sorted_ngram_scores[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How terms are used over time\n",
    "We want to visualise how terms are used over time - let's plot how many times a given term is used per year. We need to map the TFIDF matrix to a count - was the term used in a document? And then sum the counts over a time period (e.g. each year).\n",
    "\n",
    "The original dataframe has the date information..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(df.publication_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(df.publication_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: convert a matrix of term occurance into a time series of term usage...\n",
    "\n",
    "The matrix itself has a row per patent; each patent is a row in the original data frame, which contains dates (publication date and application date). Let's take a look at the publication date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.publication_date[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF to time series of term usage\n",
    "\n",
    "Good news - the way the data was captured meant that the rows are in publication date order. We can write a piece of code to group documents by year. We will score a term with a '1' for each patent that mentions this term, during a particular year. This way we record how many patents use this term per year.\n",
    "\n",
    "Now the technical part. The matrix is stored in [Compressed Sparse Row](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html) form, where each row is stored as a list of non-zero entries. This saves a lot of storage and compute as the majority of our TF-IDF matrix is 0 - and as we tend to navigate by patent, we use row compression so we can easily iterate over rows (patents). The downside is that iterating over columns is slow, as each row has to be decompressed to determine its contribution to a row.\n",
    "\n",
    "To convert the TF-IDF row entries to a 1 or 0 depending on if a term was mentioned, we simply use a condition (`tfidf[current_row_index,:] > 0`) which maps a 0 to `False` and non-zero (i.e. term was mentioned) to `True`. We then add this boolean row to a running total, where `False` adds 0 and `True` adds 1. We accumulate each patent onto a running total, until we find that a new patent refers to a new year - so we then record the total against the current year, and start a new running total from 0.\n",
    "\n",
    "As an aside, there is also [Compressed Sparse Column](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html), which would make column iteration easy - i.e. term iteration - as data would be stored in compressed columns.\n",
    "\n",
    "We also use [TQDM](https://github.com/tqdm/tqdm) which provides a nice \"wrapper\" over an iterator which provides a completion bar - useful with long running jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "\n",
    "number_of_rows, number_of_terms = tfidf.shape\n",
    "\n",
    "term_counts_per_year_csr = None\n",
    "patent_dates = df.publication_date.tolist()\n",
    "\n",
    "current_year = patent_dates[0].year\n",
    "term_counts_current_year_csr = csr_matrix((1, number_of_terms), dtype=np.int32)\n",
    "number_of_documents_per_year = []\n",
    "year_dates = []\n",
    "number_of_documents_this_year = 0\n",
    "\n",
    "for current_row_index in tqdm(range(number_of_rows), 'Calculating yearly term counts', unit='patent', total=number_of_rows):\n",
    "    new_year = patent_dates[current_row_index].year\n",
    "\n",
    "    while new_year > current_year:\n",
    "        term_counts_per_year_csr = vstack([term_counts_per_year_csr, term_counts_current_year_csr],\n",
    "                                          format='csr') if term_counts_per_year_csr is not None else term_counts_current_year_csr\n",
    "        number_of_documents_per_year.append(number_of_documents_this_year)\n",
    "        year_dates.append(current_year)\n",
    "        term_counts_current_year_csr = csr_matrix((1, number_of_terms), dtype=np.int32)\n",
    "        current_year += 1\n",
    "        number_of_documents_this_year = 0\n",
    "\n",
    "    current_row_as_counts = tfidf[current_row_index, :] > 0\n",
    "    term_counts_current_year_csr += current_row_as_counts\n",
    "    number_of_documents_this_year += 1\n",
    "\n",
    "term_counts_per_year_csr = vstack([term_counts_per_year_csr, term_counts_current_year_csr],\n",
    "                                  format='csr') if term_counts_per_year_csr is not None else term_counts_current_year_csr\n",
    "number_of_documents_per_year.append(number_of_documents_this_year)\n",
    "year_dates.append(current_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting time series\n",
    "\n",
    "Each term (n-gram) that we previously captured with TF-IDF now has a related time series; we have to extract the related column of the terms user per year count matrix (`term_counts_per_year_csr`), and plot this against the record years.\n",
    "\n",
    "To make the code more flexible, we take a required term and find where it is in the list of extracted terms, so you can quickly investigate term usage. To start with, let's look at `solar cell` which was our top listed term with the 10,000 patent sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "term_of_interest = 'solar cell'\n",
    "index_of_term_of_interest = ngram_list.index(term_of_interest)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 1.5), dpi=100)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "term_of_interest_time_series = term_counts_per_year_csr.getcol(index_of_term_of_interest).todense()\n",
    "term_of_interest_time_series = term_of_interest_time_series.flatten().tolist()[0]\n",
    "ax.plot(year_dates, term_of_interest_time_series, color='b', linestyle='-', marker='x', label='Year')\n",
    "\n",
    "ax.set_title(f'Patents using term \"{term_of_interest}\"')\n",
    "ax.set_ylabel('Number of\\npatents\\nwith term', fontsize=12)\n",
    "ax.set_xlabel('Year', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What did you find?\n",
    "Try different terms - what usage did you find? Any patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
