{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovering popular terminology within Patents\n",
    "\n",
    "This tutorial looks at the use of natural language processing to detect popular terminology within patents, and visualises the usage of such terminology over time.\n",
    "\n",
    "We will learn how to preprocess text data, transform words to numbers, convert the occurences to a time series and plot the timeseries.\n",
    "\n",
    "## What we will do:\n",
    "* Import the Python modules that will be used in the analysis.\n",
    "* Read the pre-prepared patent collections\n",
    "* Examine and discuss the data we have imported\n",
    "* Identify common terms with TF-IDF\n",
    "* Improve our results using stop words, frequency filtering and stemming\n",
    "* Identify popular terms through accumulation of TF-IDF scores\n",
    "\n",
    "\n",
    "## How is this tutorial structured:\n",
    "For every section, I will highlight its Goal and what we will do to achieve it. Then, I will explain the methods we use, what alternatives or additional thing we could do and lastly, we will run the code together. Note that some code cells can \"run\" for a while, so we will run them first and then explain what they do.\n",
    "\n",
    "## Download example patent data from PATSTAT\n",
    "\n",
    "We have already extracted a few sample datasets from the [PATSTAT](https://www.epo.org/searching-for-patents/business/patstat.html#tab-1) patents database.\n",
    "These are exported as Pandas DataFrames, so we just need to load them in.\n",
    "\n",
    "First of all, we need to prepare by loading in the support libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Collecting git+https://github.com/nestauk/im_tutorials.git\n",
      "  Cloning https://github.com/nestauk/im_tutorials.git to /private/var/folders/5w/8j3_gwlj7rdgx_rpxmfpksf40000gn/T/pip-req-build-zgipq5y_\n",
      "  Running command git clone -q https://github.com/nestauk/im_tutorials.git /private/var/folders/5w/8j3_gwlj7rdgx_rpxmfpksf40000gn/T/pip-req-build-zgipq5y_\n",
      "Requirement already satisfied (use --upgrade to upgrade): im-tutorials==0.1.0 from git+https://github.com/nestauk/im_tutorials.git in /anaconda3/envs/im_tutorials/lib/python3.7/site-packages\n",
      "Building wheels for collected packages: im-tutorials\n",
      "  Building wheel for im-tutorials (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for im-tutorials: filename=im_tutorials-0.1.0-cp37-none-any.whl size=12596 sha256=7290478e37e65629ff5f87f41a7599f5388df16e679ebc95dd8994ede30c8652\n",
      "  Stored in directory: /private/var/folders/5w/8j3_gwlj7rdgx_rpxmfpksf40000gn/T/pip-ephem-wheel-cache-iaoaeogw/wheels/47/a3/cb/bdc5f9ba49bcfd2c6864b166a1566eb2f104113bf0c3500330\n",
      "Successfully built im-tutorials\n",
      "Requirement already satisfied: smart_open in /anaconda3/envs/im_tutorials/lib/python3.7/site-packages (1.8.4)\n",
      "Requirement already satisfied: requests in /anaconda3/envs/im_tutorials/lib/python3.7/site-packages (from smart_open) (2.22.0)\n",
      "Requirement already satisfied: boto3 in /anaconda3/envs/im_tutorials/lib/python3.7/site-packages (from smart_open) (1.9.249)\n",
      "Requirement already satisfied: boto>=2.32 in /anaconda3/envs/im_tutorials/lib/python3.7/site-packages (from smart_open) (2.49.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/envs/im_tutorials/lib/python3.7/site-packages (from requests->smart_open) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /anaconda3/envs/im_tutorials/lib/python3.7/site-packages (from requests->smart_open) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /anaconda3/envs/im_tutorials/lib/python3.7/site-packages (from requests->smart_open) (1.25.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/envs/im_tutorials/lib/python3.7/site-packages (from requests->smart_open) (3.0.4)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /anaconda3/envs/im_tutorials/lib/python3.7/site-packages (from boto3->smart_open) (0.2.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /anaconda3/envs/im_tutorials/lib/python3.7/site-packages (from boto3->smart_open) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.249 in /anaconda3/envs/im_tutorials/lib/python3.7/site-packages (from boto3->smart_open) (1.12.249)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /anaconda3/envs/im_tutorials/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.249->boto3->smart_open) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /anaconda3/envs/im_tutorials/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.249->boto3->smart_open) (2.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /anaconda3/envs/im_tutorials/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.249->boto3->smart_open) (1.12.0)\n",
      "Requirement already satisfied: pandas in /anaconda3/envs/im_tutorials/lib/python3.7/site-packages (0.25.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /anaconda3/envs/im_tutorials/lib/python3.7/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /anaconda3/envs/im_tutorials/lib/python3.7/site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /anaconda3/envs/im_tutorials/lib/python3.7/site-packages (from pandas) (1.17.2)\n",
      "Requirement already satisfied: six>=1.5 in /anaconda3/envs/im_tutorials/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
      "Requirement already satisfied: scikit-learn in /anaconda3/envs/im_tutorials/lib/python3.7/site-packages (0.21.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /anaconda3/envs/im_tutorials/lib/python3.7/site-packages (from scikit-learn) (0.14.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /anaconda3/envs/im_tutorials/lib/python3.7/site-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /anaconda3/envs/im_tutorials/lib/python3.7/site-packages (from scikit-learn) (1.17.2)\n",
      "Requirement already satisfied: nltk in /anaconda3/envs/im_tutorials/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /anaconda3/envs/im_tutorials/lib/python3.7/site-packages (from nltk) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# install im_tutorial package\n",
    "!pip install git+https://github.com/nestauk/im_tutorials.git\n",
    "    \n",
    "# We also need S3 data support (to load our sample patents)\n",
    "!pip install smart_open\n",
    "\n",
    "# pandas - to manage data frames\n",
    "!pip install pandas\n",
    "\n",
    "# scikit-learn for our NLP pipeline\n",
    "!pip install scikit-learn\n",
    "\n",
    "# nltk for more NLP support (\"Natural Language ToolKit\")\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data\n",
    "\n",
    "Download the file from an S3 bucket... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from im_tutorials.data.ons import patents_10k, patents_100k\n",
    "\n",
    "df = patents_10k() \n",
    "# df = patents_100k() \n",
    "\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What have we acquired?\n",
    "Quickly check what data we've loaded... what attributes are available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['appln_id', 'abstract', 'appln_auth', 'application_date',\n",
       "       'application_id', 'publication_date', 'patent_id',\n",
       "       'applicant_countries', 'applicant_cities', 'inventor_countries',\n",
       "       'inventor_cities', 'invention_title', 'classifications_cpc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example patent?\n",
    "What does the a random entry look like? Let's take a look at row 500..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "appln_id                                                        33191383\n",
       "abstract               PURPOSE:To miniaturize a hot water storage tan...\n",
       "appln_auth                                                            JP\n",
       "application_date                                     1979-12-18 00:00:00\n",
       "application_id                                                  54165040\n",
       "publication_date                                     1981-07-16 00:00:00\n",
       "patent_id                                                       15804685\n",
       "applicant_countries                                                  NaN\n",
       "applicant_cities                                                     NaN\n",
       "inventor_countries                                                   NaN\n",
       "inventor_cities                                                      NaN\n",
       "invention_title        HOT WATER STORAGE TYPE WATER HEATER UTILIZING ...\n",
       "classifications_cpc                                        [Y02E  10/40]\n",
       "Name: 33191383, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking for popular terminology\n",
    "\n",
    "We will use TF-IDF to find statistically popular terminology - where \"terminology\" is defined as a sequence of words. \n",
    "\n",
    "## TF-IDF\n",
    "\n",
    "TF-IDF is defined as \"Term Frequency - Inverse Document Frequency\", where the frequeny of a term in a document is divided by the number of documents it occurs in. This \"normalises\" a popular term by reducing its popularity by dividing by the number of documents it occurs in - if every document uses this term, it isn't very unusual, more likely to be a word such as \"the\" or \"and\".\n",
    "\n",
    "We use scikit-learn's implementation of TFIDF (refer to their [example of topic extraction](https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.htm) which uses TFIDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed in 1.16s.\n",
      "Number of features (words in our dictionary): 36,718\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "from time import time\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(df.abstract)\n",
    "print(f\"Processed in {time() - t0:.2f}s.\")\n",
    "print(f'Number of features (words in our dictionary): {len(tfidf_vectorizer.get_feature_names()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfiltered results\n",
    "\n",
    "What words have we discovered? Let's look at the first 10 terms or \"feature names\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '00000',\n",
       " '00001',\n",
       " '00002',\n",
       " '0001',\n",
       " '0005',\n",
       " '000angstrom',\n",
       " '000deg',\n",
       " '000kg']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's a lot of 0's\n",
    "\n",
    "Oh dear. Maybe we should remove digits and punctuation? Let's just keep A-Z (assuming we are restricted to English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed in 1.06s.\n",
      "Number of features (words in our dictionary): 30,550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'aa',\n",
       " 'ab',\n",
       " 'aback',\n",
       " 'abaissement',\n",
       " 'abaisser',\n",
       " 'abajo',\n",
       " 'abamectin',\n",
       " 'abandon',\n",
       " 'abandoned']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(token_pattern=r'[A-Za-z]+', analyzer='word')\n",
    "\n",
    "from time import time\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(df.abstract)\n",
    "print(f\"Processed in {time() - t0:.2f}s.\")\n",
    "print(f'Number of features (words in our dictionary): {len(tfidf_vectorizer.get_feature_names()):,}')\n",
    "tfidf_vectorizer.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just single words\n",
    "Looks better, but isolated words aren't very useful - no context. How about pairs or triplets of words? (bi-grams and tri-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed in 8.96s.\n",
      "Number of features (words in our dictionary): 1,208,186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a a',\n",
       " 'a a a',\n",
       " 'a a and',\n",
       " 'a a are',\n",
       " 'a a b',\n",
       " 'a a beigemischt',\n",
       " 'a a belt',\n",
       " 'a a bivalent',\n",
       " 'a a block',\n",
       " 'a a bottom']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(token_pattern=r'[A-Za-z]+', analyzer='word', ngram_range=(2,3))\n",
    "\n",
    "from time import time\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(df.abstract)\n",
    "print(f\"Processed in {time() - t0:.2f}s.\")\n",
    "print(f'Number of features (words in our dictionary): {len(tfidf_vectorizer.get_feature_names()):,}')\n",
    "tfidf_vectorizer.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-grams and tri-grams\n",
    "Yikes! That didn't help! Mind you \"a\" isn't a very useful word. Let's add in some \"stopwords\"..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed in 7.15s.\n",
      "Number of features (words in our dictionary) after English stop words removed: 1,147,284 bigrams and trigrams\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['aa anode',\n",
       " 'aa anode bb',\n",
       " 'aa base',\n",
       " 'aa base material',\n",
       " 'aa cc',\n",
       " 'aa cc reducing',\n",
       " 'aa comparative',\n",
       " 'aa comparative example',\n",
       " 'aa cooling',\n",
       " 'aa cooling water']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(token_pattern=r'[A-Za-z]+', analyzer='word', \n",
    "                                   ngram_range=(2,3), stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(df.abstract)\n",
    "print(f\"Processed in {time() - t0:.2f}s.\")\n",
    "print(f'Number of features (words in our dictionary) after English stop words removed: {len(tfidf_vectorizer.get_feature_names()):,} bigrams and trigrams')\n",
    "tfidf_vectorizer.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unusual terms still present\n",
    "Hmmn. What if we skip rare terms, that could just be formatting or spelling errors? How about only terms that occur in at least 5 documents..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed in 6.75s.\n",
      "...after English stop words removed, remove terms occuring in less than 5 documents: 15,961 bigrams and trigrams\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['abh ngigkeit',\n",
       " 'abnormality detection',\n",
       " 'abrasion resistance',\n",
       " 'absolute value',\n",
       " 'absorb heat',\n",
       " 'absorber layer',\n",
       " 'absorber plate',\n",
       " 'absorber second',\n",
       " 'absorber solution',\n",
       " 'absorbing device']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimum_document_frequency = 5\n",
    "tfidf_vectorizer = TfidfVectorizer(token_pattern=r'[A-Za-z]+', analyzer='word', \n",
    "                                   ngram_range=(2,3), stop_words='english', \n",
    "                                   min_df=minimum_document_frequency)\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(df.abstract)\n",
    "print(f\"Processed in {time() - t0:.2f}s.\")\n",
    "print(f'...after English stop words removed, remove terms occuring in less than {minimum_document_frequency} documents:'\n",
    "      f' {len(tfidf_vectorizer.get_feature_names()):,} bigrams and trigrams')\n",
    "tfidf_vectorizer.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meaningful bi- and tri-grams\n",
    "That's better! That's really reduced the number of n-grams. What else have we got?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['absorbing heat',\n",
       " 'absorbing layer',\n",
       " 'absorbing material',\n",
       " 'absorbing solar',\n",
       " 'absorbing surface',\n",
       " 'absorbs heat',\n",
       " 'absorption coating',\n",
       " 'absorption efficiency',\n",
       " 'absorption heat',\n",
       " 'absorption heat pump']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names()[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same words, different forms?\n",
    "Hmmn. That's a lot of variants of 'absorb'. If we had a \"stemmer\" we could remove common endings to get to the common \"stem\" (note that this is different to lemmatising - lemmas are the basic form of the word, but require a dictionary - patent words might not all be in the dictionary).\n",
    "\n",
    "First of all, let's load NLTK's library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/grimsi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A \"stemming\" tokenizer\n",
    "\n",
    "We need a piece of code that can extract words (\"tokens\") from a stream of text - and \"stem\" the words..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['absorb', 'absorb', 'absorb', 'absorpt', '123']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "class StemTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.ps = nltk.PorterStemmer()\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        return [self.ps.stem(t) for t in word_tokenize(doc)]\n",
    "\n",
    "t = StemTokenizer()\n",
    "t('absorbs absorbing absorber absorption 123')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming Tokenizer ready\n",
    "\n",
    "Looks good, multiple forms of \"absorb\" are now mapped to a single stem - shame about \"absorption\" - a lemmatiser could map this to \"absorb\" if it was in the lemmatiser's dictionary.\n",
    "\n",
    "Never mind, let's try it with the patent abstracts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/im_tutorials/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed in 45.60s.\n",
      "...after English stop words removed, remove terms occuring in less than 5 documents: 28,334 bigrams and trigrams\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['! 2.',\n",
       " '! 3.',\n",
       " '% (',\n",
       " '% )',\n",
       " '% ) .',\n",
       " '% ,',\n",
       " '% -10',\n",
       " '% -10 %',\n",
       " '% -15',\n",
       " '% -15 %']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimum_document_frequency = 5\n",
    "tfidf_vectorizer = TfidfVectorizer(token_pattern=r'[A-Za-z]+', analyzer='word', \n",
    "                                   ngram_range=(2,3), stop_words='english', \n",
    "                                   min_df=minimum_document_frequency,\n",
    "                                   tokenizer=StemTokenizer())\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(df.abstract)\n",
    "print(f\"Processed in {time() - t0:.2f}s.\")\n",
    "print(f'...after English stop words removed, remove terms occuring in less than {minimum_document_frequency} documents:'\n",
    "      f' {len(tfidf_vectorizer.get_feature_names()):,} bigrams and trigrams')\n",
    "tfidf_vectorizer.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What went wrong?\n",
    "\n",
    "Oh dear. Tokenizer overrides the regular expression, so we'll have to combine the two..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['absorb', 'absorb', 'absorb', 'absorpt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "class StemTokenizerWithWordFilter(object):\n",
    "    def __init__(self):\n",
    "        self.ps = nltk.PorterStemmer()\n",
    "        self.token_pattern = re.compile(r'[A-Za-z]+')\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        return [self.ps.stem(t) for t in self.token_pattern.findall(doc)]\n",
    "\n",
    "t = StemTokenizerWithWordFilter()\n",
    "t('absorbs absorbing absorber absorption 123')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemmer revisited\n",
    "\n",
    "Great - digits are removed, and the \"absorb\" stemming still works - so let's try again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/im_tutorials/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed in 34.33s.\n",
      "...after English stop words removed, remove terms occuring in less than 5 documents: 21,641 bigrams and trigrams\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['abh ngigkeit',\n",
       " 'abnorm detect',\n",
       " 'abov atmospher',\n",
       " 'abov deg',\n",
       " 'abov deg c',\n",
       " 'abov describ',\n",
       " 'abov heat',\n",
       " 'abov mention',\n",
       " 'abov predetermin',\n",
       " 'abov process']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimum_document_frequency = 5\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', \n",
    "                                   ngram_range=(2,3), stop_words='english',\n",
    "                                   min_df=minimum_document_frequency,\n",
    "                                   tokenizer=StemTokenizerWithWordFilter())\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(df.abstract)\n",
    "print(f\"Processed in {time() - t0:.2f}s.\")\n",
    "print(f'...after English stop words removed, remove terms occuring in less than {minimum_document_frequency} documents:'\n",
    "      f' {len(tfidf_vectorizer.get_feature_names()):,} bigrams and trigrams')\n",
    "tfidf_vectorizer.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errors from scikit-learn?\n",
    "\n",
    "Ah - yes, we are comparing stemmed words with the original stopword list which isn't stemmed. Whoops. Let's stem the stopwords so they will match the output of the stemmer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/grimsi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['until', 'becaus', 'other', 'thi', 'the', 'my', 'and', 'needn', 'both', 'few']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words_as_string = \" \".join(stop_words)\n",
    "stemmed_stop_words = StemTokenizerWithWordFilter()(stop_words_as_string)\n",
    "stemmed_stop_words_no_duplicates = list(set(stemmed_stop_words))\n",
    "stemmed_stop_words_no_duplicates[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemmed stopwords\n",
    "Let's analyse the patents again, this time with the stopwords matching the output of a our stemmer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/im_tutorials/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['becau'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed in 34.73s.\n",
      "...after English stop words removed, remove terms occuring in less than 5 documents: 22,860 bigrams and trigrams\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['abh ngigkeit',\n",
       " 'abnorm detect',\n",
       " 'abras resist',\n",
       " 'absolut valu',\n",
       " 'absorb absorb',\n",
       " 'absorb absorpt',\n",
       " 'absorb devic',\n",
       " 'absorb first',\n",
       " 'absorb heat',\n",
       " 'absorb heat heat']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimum_document_frequency = 5\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', \n",
    "                                   ngram_range=(2,3), \n",
    "                                   stop_words=stemmed_stop_words_no_duplicates,\n",
    "                                   min_df=minimum_document_frequency,\n",
    "                                   tokenizer=StemTokenizerWithWordFilter())\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(df.abstract)\n",
    "print(f\"Processed in {time() - t0:.2f}s.\")\n",
    "print(f'...after English stop words removed, remove terms occuring in less than {minimum_document_frequency} documents:'\n",
    "      f' {len(tfidf_vectorizer.get_feature_names()):,} bigrams and trigrams')\n",
    "tfidf_vectorizer.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeated words?\n",
    "\n",
    "Hmmn. Slightly odd - \"absorb heat heat\" etc.; let's see what else we have... let's look at the following 10 terms..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['absorb high',\n",
       " 'absorb layer',\n",
       " 'absorb light',\n",
       " 'absorb liquid',\n",
       " 'absorb materi',\n",
       " 'absorb pipe',\n",
       " 'absorb plate',\n",
       " 'absorb refriger',\n",
       " 'absorb second',\n",
       " 'absorb solar']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names()[10:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, not so bad after all. Hopefully we've now got a sensible feature set - what features are of interest?\n",
    "\n",
    "# Features of interest\n",
    "\n",
    "One approach is to look at the TFIDF matrix; each row represents a document, each column a feature (i.e. an \"n-gram\"). A feature is of interest if it is popular and interesting - by that we mean it appears repeatedly in a document but not in all documents. Or, in other words, a high TF-IDF value against a term.\n",
    "\n",
    "Let's try collapsing the matrix by summing the rows; this will reveal which features have the highest weights and in turn which n-grams are of interest..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 22860)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed_tfidf = tfidf.sum(axis=0)\n",
    "summed_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which term accumulated what TF-IDF total?\n",
    "Let's associate the n-grams with their scores..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22860\n",
      "22860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1.964984642007418, 'abh ngigkeit'),\n",
       " (1.9833368916355836, 'abnorm detect'),\n",
       " (2.1418959799734973, 'abras resist'),\n",
       " (1.8430863781922602, 'absolut valu'),\n",
       " (0.8510145638607169, 'absorb absorb'),\n",
       " (0.7004883503700028, 'absorb absorpt'),\n",
       " (0.8188876183620329, 'absorb devic'),\n",
       " (0.5389650133241702, 'absorb first'),\n",
       " (3.6754122614917653, 'absorb heat'),\n",
       " (0.6217575810475298, 'absorb heat heat')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed_tfidf_list = summed_tfidf.tolist()[0]\n",
    "print(len(summed_tfidf_list))\n",
    "\n",
    "ngram_list = tfidf_vectorizer.get_feature_names()\n",
    "print(len(ngram_list))\n",
    "\n",
    "ngram_scores = list(zip(summed_tfidf_list, tfidf_vectorizer.get_feature_names()))\n",
    "ngram_scores[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which terms have the highest accumulated TF-iDF score?\n",
    "So if we sort the tuples by TF-IDF accumulated score..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(87.59415244080746, 'util model'),\n",
       " (86.23965268576134, 'least one'),\n",
       " (84.13185378317797, 'problem solv'),\n",
       " (70.4916842323881, 'solar cell'),\n",
       " (68.95704829225679, 'heat exchang'),\n",
       " (66.66387292986626, 'power gener'),\n",
       " (65.03648010606014, 'invent relat'),\n",
       " (64.17028054759605, 'present invent'),\n",
       " (60.038667850465764, 'solv provid'),\n",
       " (60.017751716429366, 'problem solv provid'),\n",
       " (58.10918030547546, 'power suppli'),\n",
       " (52.437059533307604, 'exhaust ga'),\n",
       " (51.715616871843, 'invent disclos'),\n",
       " (49.485036218829784, 'raw materi'),\n",
       " (45.15404079848826, 'model disclos'),\n",
       " (45.15404079848826, 'util model disclos'),\n",
       " (44.27394436080138, 'p solut'),\n",
       " (44.21824454019031, 'c jpo'),\n",
       " (44.21824454019031, 'copyright c'),\n",
       " (44.21824454019031, 'copyright c jpo')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_ngram_scores = sorted(ngram_scores, key=lambda tup: tup[0], reverse=True)\n",
    "sorted_ngram_scores[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have popular terminology! Is it meaningful?\n",
    "\n",
    "Now we're getting somewhere! However, there are a number of n-grams that aren't useful:\n",
    "* util model (\"utility model\"?)\n",
    "* least one (\"...at least one...\"?)\n",
    "* invent relat (\"invention related\"?)\n",
    "* present invent (\"present invention\"?)\n",
    "* invent disclos (\"invention disclosed\"?)\n",
    "\n",
    "Suggest we add \"invention\" to the stopword list...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['are',\n",
       " 'an',\n",
       " 'invent',\n",
       " 'util',\n",
       " 'disclos',\n",
       " 'problem',\n",
       " 'solv',\n",
       " 'becau',\n",
       " 'copyright',\n",
       " 'one']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_stop_words_custom = stemmed_stop_words_no_duplicates + ['invent', 'util', 'disclos', 'problem', 'solv', 'becau', 'copyright', 'one']\n",
    "stemmed_stop_words_custom[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerun with revised stopwords\n",
    "Let's try again, with the revised list of words to ignore..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/im_tutorials/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['disclo'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed in 34.49s.\n",
      "...after English stop words removed, remove terms occuring in less than 5 documents: 22,283 bigrams and trigrams\n",
      "22283\n",
      "22283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(71.59628262339267, 'solar cell'),\n",
       " (69.58197938536033, 'heat exchang'),\n",
       " (67.55062628438021, 'power gener'),\n",
       " (58.65742995494544, 'power suppli'),\n",
       " (52.89986476139019, 'exhaust ga'),\n",
       " (50.31499614602553, 'raw materi'),\n",
       " (45.51062668368377, 'p c'),\n",
       " (45.50908955529698, 'p solut'),\n",
       " (45.45388738177465, 'c jpo'),\n",
       " (45.45388738177465, 'p c jpo'),\n",
       " (44.30474600746714, 'combust engin'),\n",
       " (44.15775185053338, 'deg c'),\n",
       " (42.1165349730431, 'intern combust'),\n",
       " (41.30885745905931, 'intern combust engin'),\n",
       " (37.70586353655121, 'combust chamber'),\n",
       " (36.113257106080034, 'e g'),\n",
       " (34.746712203393855, 'high temperatur'),\n",
       " (34.62581249743111, 'solar energi'),\n",
       " (34.14698368013074, 'water tank'),\n",
       " (33.827613467761225, 'control system')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimum_document_frequency = 5\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', \n",
    "                                   ngram_range=(2,3), \n",
    "                                   stop_words=stemmed_stop_words_custom,\n",
    "                                   min_df=minimum_document_frequency,\n",
    "                                   tokenizer=StemTokenizerWithWordFilter())\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(df.abstract)\n",
    "print(f\"Processed in {time() - t0:.2f}s.\")\n",
    "print(f'...after English stop words removed, remove terms occuring in less than {minimum_document_frequency} documents:'\n",
    "      f' {len(tfidf_vectorizer.get_feature_names()):,} bigrams and trigrams')\n",
    "\n",
    "summed_tfidf = tfidf.sum(axis=0)\n",
    "summed_tfidf_list = summed_tfidf.tolist()[0]\n",
    "print(len(summed_tfidf_list))\n",
    "\n",
    "ngram_list = tfidf_vectorizer.get_feature_names()\n",
    "print(len(ngram_list))\n",
    "\n",
    "ngram_scores = list(zip(summed_tfidf_list, tfidf_vectorizer.get_feature_names()))\n",
    "sorted_ngram_scores = sorted(ngram_scores, key=lambda tup: tup[0], reverse=True)\n",
    "sorted_ngram_scores[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How terms are used over time\n",
    "We want to visualise how terms are used over time - let's plot how many times a given term is used per year. We need to map the TFIDF matrix to a count - was the term used in a document? And then sum the counts over a time period (e.g. each year).\n",
    "\n",
    "The original dataframe has the date information..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('1913-07-24 00:00:00')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(df.publication_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2017-07-27 00:00:00')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df.publication_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_rows, number_of_terms = tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22283"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
