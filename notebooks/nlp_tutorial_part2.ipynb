{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Tutorial\n",
    "\n",
    "### Elena Kochkina\n",
    "\n",
    "NESTA HackSTIR\n",
    "\n",
    "22.10.2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('reuters')\n",
    "from nltk.corpus import reuters\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gensim\n",
    "import numpy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reuters dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "reuters.categories()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting training and testing sets of reuters dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "documents_train = []\n",
    "labels_train = []\n",
    "documents_test = []\n",
    "labels_test = []\n",
    "#categories = reuters.categories()\n",
    "categories = ['wheat','gold','ship','coffee','grain']\n",
    "for cat in categories:\n",
    "  print (cat)\n",
    "  print (len(reuters.fileids(cat)))\n",
    "  for fileid in reuters.fileids(cat):\n",
    "    if fileid.startswith('training'):\n",
    "      documents_train.append(reuters.raw(fileid))\n",
    "      labels_train.append(cat)\n",
    "    else:\n",
    "      documents_test.append(reuters.raw(fileid))\n",
    "      labels_test.append(cat)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(len(documents_train))\n",
    "print(documents_train[0])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "documents_train_preprocessed = []\n",
    "for d in documents_train:\n",
    "  newd = d.lower()\n",
    "  newd = re.sub(r'[^A-Za-z0-9 ]+', '', newd)\n",
    "  documents_train_preprocessed.append(newd)\n",
    "  \n",
    "documents_test_preprocessed = []\n",
    "for d in documents_test:\n",
    "  newd = d.lower()\n",
    "  newd = re.sub(r'[^A-Za-z0-9 ]+', '', newd)\n",
    "  documents_test_preprocessed.append(newd)\n",
    "  "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_train_bow = vectorizer.fit_transform(documents_train_preprocessed)\n",
    "X_test_bow = vectorizer.transform(documents_test_preprocessed)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier w BoW features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(X_train_bow, labels_train)\n",
    "Y_pred_bow = clf.predict(X_test_bow)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(accuracy_score(labels_test, Y_pred_bow))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "downloading word2vec model pre-trained on Google News corpus"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
    "!gzip -d GoogleNews-vectors-negative300.bin.gz\n",
    "!ls"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def preprocess_to_avgw2v(documents):\n",
    "  documents_avgw2v = []\n",
    "  for d in documents:\n",
    "    words = nltk.word_tokenize(d)\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    words = [w for w in words if not w in stops]\n",
    "    num_features = 300  \n",
    "    temp_rep = numpy.zeros(num_features)\n",
    "\n",
    "    for w in words:\n",
    "      if w in model:\n",
    "        temp_rep+=model[w]\n",
    "      \n",
    "    sumw2v = temp_rep/len(words)\n",
    "    documents_avgw2v.append(sumw2v)\n",
    "      \n",
    "  return documents_avgw2v\n",
    "    "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X_train_w2v = preprocess_to_avgw2v(documents_train_preprocessed)\n",
    "X_test_w2v = preprocess_to_avgw2v(documents_test_preprocessed)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier w Word2vec features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(X_train_w2v, labels_train)\n",
    "Y_pred_w2v = clf.predict(X_test_w2v)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(accuracy_score(labels_test, Y_pred_w2v))"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
