{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subject Detection with Topic Cooccurrence Networks - Gateway to Research\n",
    "\n",
    "This tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing useful Python utility libraries we'll need\n",
    "import ast\n",
    "import smart_open\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "\n",
    "# matplotlib for static plots\n",
    "import matplotlib.pyplot as plt\n",
    "# numpy for mathematical functions\n",
    "import numpy as np\n",
    "# pandas for handling tabular data\n",
    "import pandas as pd\n",
    "\n",
    "from im_tutorials.utilities import chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket='innovation-mapping-tutorials'\n",
    "gtr_projects_key='gateway-to-re'\n",
    "list_cols = ['research_topics', 'research_subjects']\n",
    "# We use ast.literal_eval to convert the two columns above from\n",
    "# string representations of lists to actual lists.\n",
    "gtr_projects_df = pd.read_csv(\n",
    "    smart_open.smart_open('s3://innovation-mapping-tutorials/gateway-to-research/gtr_projects.csv'),\n",
    "    converters={k: ast.literal_eval for k in list_cols}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_projects_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the lists of research subjects and elements and count the contents\n",
    "research_subject_counter = Counter(chain(*gtr_projects_df['research_subjects']))\n",
    "research_topic_counter = Counter(chain(*gtr_projects_df['research_topics']))\n",
    "print('There are {} unique research subjects in the GtR projects dataset.'.format(len(research_subject_counter)))\n",
    "print('There are {} unique research topics in the GtR projects dataset.'.format(len(research_topic_counter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top Research Topics by Frequency\", '\\n')\n",
    "print('{:<40}{}'.format('Topic', 'Frequency'))\n",
    "for k, v in research_topic_counter.most_common(20):\n",
    "    print('{:<40}{}'.format(k, v))\n",
    "    \n",
    "print('\\nMedian Topic Freqency:')\n",
    "print(np.median(list(research_topic_counter.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the top research topic is _Climate & Climate Change_ by some margin. However, we can also see that the top spots are populated by topics from several disciplines. 50% of the topics occur 69 times or fewer, again highlighting the skewness of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field Definition Through Community Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to define communities of research topics as groups of topics which commonly occur together. An effective way of finding these clusters, and visualising the results, is by creating a topic cooccurrence graph.\n",
    "\n",
    "A cooccurrence graph is a network structure, where nodes are elements and an edge represents the elements of two nodes having cooccured at least once. The edges can then be \"weighted\" by the frequencies of each cooccurring pair. In the case of our research projects, we can say that two topics have cooccurred if they appear in at least one project together. To find all cooccurrences we therefore need to find the pairwise combinations of research topics for every project. For example, a single project with the topics\n",
    "```\n",
    "['Materials Characterisation', 'High Performance Computing', 'Condensed Matter Physics']\n",
    "```\n",
    "\n",
    "will become a set of topic pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The combinations function from itertools generates all the possible\n",
    "# elements of combinations from a list with length  r.\n",
    "list(combinations(['Materials Characterisation', 'High Performance Computing', 'Condensed Matter Physics'], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These cooccurrences would form a triangular network, where each edge has a frequency weight of 1.\n",
    "\n",
    "To create a cooccurrence network across all projects, we need to repeat this process for every project. We can do this in a Python list comprehension, and then chain togeher all of the cooccurring pairs into one long list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate every pairwise combination of research topics from each project.\n",
    "# Each pair is sorted alphabetically to make sure that there is only one \n",
    "# possible permutation of each edge.\n",
    "cooccurrences = list(chain(*[[tuple(sorted(c)) for c in (itertools.combinations(d, 2))] for d in gtr_projects_df['research_topics']]))\n",
    "# Count the frequency of each cooccurring pair.\n",
    "research_topic_co_counter = Counter(cooccurrences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top Research Topic Cooccurrences by Frequency\", '\\n')\n",
    "print('{:<70}{}'.format('Cooccurrence', 'Frequency'))\n",
    "for k, v in research_topic_co_counter.most_common(20):\n",
    "    print('{:<70}{}'.format((k[0] + ' + ' +k[1]), v))\n",
    "    \n",
    "print('\\nMedian Topic Cooccurrence Freqency:')\n",
    "print(np.median(list(research_topic_co_counter.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the most frequently cooccurring topics we can pairs that make intuitive sense and are all generally captured neatly within higher order academic disciplines.\n",
    "\n",
    "However this, along with the individual topic frequencies, also shows us that using the cooccurrence frequency as our edge weight might not be such a good idea. High frequency elements are simply more likely to cooccur due to chance. Therefore we should normalise our edge weights. One method for this is to calculate the association strength, a proababilistic measure, where the cooccurrence freqency is normalised by the product of the individual terms' occurrence counts. It is defined as\n",
    "\n",
    "$$ a = \\frac{2 n c_{ij}}{o_{i}o_{j}} $$\n",
    "\n",
    "where $n$ is the total number of elements, $c_{ij}$ is the number of cooccurrences between elements $i$ and $j$, and $o_{i}$ and $o_{j}$ are the individual frequency counts of each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def association_strength(combo, occurrences, cooccurrences, total):\n",
    "    '''association_strength\n",
    "    Calculates the association strength between a cooccurring pair.\n",
    "    '''\n",
    "    a_s = ((2 * total * cooccurrences[combo]) / \n",
    "           (occurrences[combo[0]] * occurrences[combo[1]]))\n",
    "    return a_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build our cooccurrence network, we need to generate a list of unique edges from our long list of cooccurrences and then calculate the association strength for each edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a set of cooccurences (a list of unique pairs).\n",
    "# This will form the edges of our cooccurrence graph.\n",
    "edges = set(cooccurrences)\n",
    "# Calculate the total number of elements\n",
    "n = len(list(chain(*gtr_projects_df['research_topics'])))\n",
    "# Calculate the association strength for each edge.\n",
    "# We take the log of the association strength to give it\n",
    "# a normal distribution.\n",
    "assoc_strengths = np.log10([association_strength(\n",
    "    edge,\n",
    "    research_topic_counter, \n",
    "    research_topic_co_counter, \n",
    "    n) for edge in edges])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(assoc_strengths, bins=100)\n",
    "ax.set_xlabel('Association Strength')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_edges = []\n",
    "for (s, t), a_s in zip(edges, assoc_strengths):\n",
    "    weighted_edges.append((s, t, a_s))\n",
    "\n",
    "# for (s, t), count in research_topic_co_counter.items():\n",
    "#     weighted_edges.append((s, t, count))\n",
    "\n",
    "g = nx.Graph()\n",
    "g.add_weighted_edges_from(weighted_edges, weight='association_strength')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g.edges[('Materials Characterisation', 'Materials Synthesis & Growth')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = community.best_partition(g, resolution=0.6, random_state=42, weight='weight')\n",
    "n_communities = len(set(part.values()))\n",
    "print('{} communities detected.'.format(n_communities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Network Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we add some extra properties to the nodes in our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.palettes import Category20, Spectral4\n",
    "from bokeh.models import Circle, MultiLine, HoverTool, TapTool\n",
    "from bokeh.models.graphs import from_networkx, NodesAndLinkedEdges\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {k: k for k, _ in part.items()}\n",
    "nx.set_node_attributes(g, names, name='topic_name')\n",
    "community_colors = {k: Category20[n_communities][c] for k, c in part.items()}\n",
    "nx.set_node_attributes(g, community_colors, name='color')\n",
    "\n",
    "print(g.nodes['Materials Characterisation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we calculate positions for the visual graph layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(g, weight='association_strength', scale=2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bokeh` has built-in support for `networkx` graphs, which makes plotting and interacting with them easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = figure(title=\"Research Topic Cooccurrence Network\",\n",
    "              x_range=(-2.1,2.1), y_range=(-2.1,2.1),\n",
    "             )\n",
    "\n",
    "graph_renderer = from_networkx(g, pos, center=(0,0))\n",
    "graph_renderer.node_renderer.glyph = Circle(size=7, fill_color='color', line_color=None)\n",
    "graph_renderer.node_renderer.selection_glyph = Circle(size=7, fill_color='color')\n",
    "graph_renderer.node_renderer.hover_glyph = Circle(size=7, fill_color='color')\n",
    "graph_renderer.node_renderer.muted_glyph = Circle(size=7, fill_color='color', fill_alpha=0.9)\n",
    "\n",
    "\n",
    "graph_renderer.edge_renderer.glyph = MultiLine(line_color=\"#CCCCCC\", line_alpha=0.2, line_width=1)\n",
    "graph_renderer.edge_renderer.selection_glyph = MultiLine(line_color=Spectral4[2], line_width=1.5)\n",
    "graph_renderer.edge_renderer.hover_glyph = MultiLine(line_color=Spectral4[1], line_width=1.5)\n",
    "\n",
    "graph_renderer.selection_policy = NodesAndLinkedEdges()\n",
    "\n",
    "node_hover_tool = HoverTool(tooltips=[(\"Topic\", \"@topic_name\")])\n",
    "plot.add_tools(node_hover_tool, TapTool())\n",
    "\n",
    "plot.renderers.append(graph_renderer)\n",
    "\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the Communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's manually inspect the topics in each community to see if we can see what disciplines they might form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_part = defaultdict(list)\n",
    "for k, v in part.items():\n",
    "    reverse_part[v].append(k)\n",
    "    \n",
    "for c, topics in reverse_part.items():\n",
    "    print(c)\n",
    "    for chunk in chunks(topics, 4):\n",
    "        print(', '.join(chunk))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a community ID to discipline mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_discipline_map = {\n",
    "    0: 'social_sciences',\n",
    "    1: 'arts_humanities',\n",
    "    2: 'environmental_sciences',\n",
    "    3: 'maths_computing_ee',\n",
    "    4: 'social_sciences',\n",
    "    5: 'biological_sciences',\n",
    "    6: 'physics_chemistry_engineering',\n",
    "    7: 'astro_particle_physics',\n",
    "    8: 'social_sciences'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_discipline_mapping = {top: community_discipline_map[disc] for top, disc in part.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning Subjects to Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map topics to disciplines using pandas' apply method on\n",
    "# the `research_topics` column.\n",
    "gtr_projects_df['disciplines'] = gtr_projects_df['research_topics'].apply(\n",
    "    lambda x: [topic_discipline_mapping[val] for val in x])\n",
    "# \n",
    "gtr_projects_df['discipline_set'] = [set(d) for d in gtr_projects_df['disciplines']]\n",
    "gtr_projects_df['discipline_set'][\n",
    "    (gtr_projects_df['funder_name'] == 'MRC') | \n",
    "    (gtr_projects_df['funder_name'] == 'NC3Rs')] = set(['medical_sciences'])\n",
    "# \n",
    "gtr_projects_df['n_disciplines'] = [len(x) for x in gtr_projects_df['discipline_set']]\n",
    "# \n",
    "gtr_projects_df['is_single_discipline'] = [True if len(x)==1 else np.nan if len(x)==0 else False \n",
    "                                           for x in gtr_projects_df['discipline_set']]\n",
    "\n",
    "gtr_projects_df['is_single_discipline'].mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discipline_cooccurrences = list(\n",
    "    chain(*[[tuple(sorted(c)) for c in itertools.combinations_with_replacement(d, 2)] for d in gtr_projects_df['discipline_set']])\n",
    ")\n",
    "# Count the frequency of each cooccurring pair.\n",
    "discipline_edge_counter = Counter(discipline_cooccurrences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discipline_cooccurrence_df = pd.DataFrame({\n",
    "    'subj0': [dcc[0] for dcc in discipline_edge_counter.keys()],\n",
    "    'subj1': [dcc[1] for dcc in discipline_edge_counter.keys()],\n",
    "    'count': list(discipline_edge_counter.values()),\n",
    "}).pivot_table(index='subj0', columns='subj1')['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_discipline_labels(labels):\n",
    "    return [l.get_text().replace('_', ' ').title() for l in labels]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "sns.heatmap(discipline_cooccurrence_df, annot=True, fmt='.0f', ax=ax, cbar=None)\n",
    "ax.set_xticklabels(format_discipline_labels(ax.get_xticklabels()), rotation=30, ha='right')\n",
    "ax.set_yticklabels(format_discipline_labels(ax.get_yticklabels()))\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel(None)\n",
    "ax.set_ylabel(None)\n",
    "ax.set_title('Discipline Crossover in Multidiscplinary Projects')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "gtr_projects_df['n_disciplines'].value_counts().plot.bar(color='C0', ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommunityPartition:\n",
    "    def __init__(self, graph):\n",
    "        self.graph = graph\n",
    "    \n",
    "    def edgelist_to_cooccurrence(self, repeats, **best_partition_kwargs):\n",
    "        edge_counter = Counter()\n",
    "        for i in range(repeats):\n",
    "            partition = community.best_partition(self.graph, random_state=i, **best_partition_kwargs)\n",
    "            edgelist = self.partition_to_edgelist(partition)\n",
    "            edge_counter.update(edgelist)\n",
    "\n",
    "        g = nx.Graph()\n",
    "        g.add_weighted_edges_from([(e[0][0], e[0][1], e[1]) for e in edge_counter.items()])\n",
    "        return g\n",
    "    \n",
    "    def partition_to_edgelist(self, partition):\n",
    "        partition_reverse_mapping = self.reverse_index_partition(partition)\n",
    "        edgelist = []\n",
    "        for community, elements in partition_reverse_mapping.items():\n",
    "            combos = [tuple(sorted(e)) for e in itertools.combinations(elements, 2)]\n",
    "            edgelist.extend(combos)\n",
    "        return edgelist\n",
    "     \n",
    "    def reverse_index_partition(self, partition):\n",
    "        partition_reverse_mapping = defaultdict(list)\n",
    "        for k, v in partition.items():\n",
    "            partition_reverse_mapping[v].append(k)\n",
    "        return partition_reverse_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = CommunityPartition(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_co = cp.edgelist_to_cooccurrence(5, resolution=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_c_co = community.best_partition(c_co, resolution=0.4, random_state=42, weight='weight')\n",
    "n_c_co_communities = len(set(part_c_co.values()))\n",
    "print('{} communities detected.'.format(n_c_co_communities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {k: k for k, _ in part.items()}\n",
    "nx.set_node_attributes(c_co, names, name='topic_name')\n",
    "c_co_community_colors = {k: Category20[n_c_co_communities][c] for k, c in part_c_co.items()}\n",
    "nx.set_node_attributes(c_co, c_co_community_colors, name='color')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(c_co, weight='weight', scale=2, random_state=42)\n",
    "\n",
    "plot = figure(title=\"Research Topic Cooccurrence Network\",\n",
    "              x_range=(-2.1,2.1), y_range=(-2.1,2.1),\n",
    "             )\n",
    "\n",
    "graph_renderer = from_networkx(c_co, pos, center=(0,0))\n",
    "graph_renderer.node_renderer.glyph = Circle(size=7, fill_color='color', line_color=None)\n",
    "graph_renderer.node_renderer.selection_glyph = Circle(size=7, fill_color='color')\n",
    "graph_renderer.node_renderer.hover_glyph = Circle(size=7, fill_color='color')\n",
    "graph_renderer.node_renderer.muted_glyph = Circle(size=7, fill_color='color', fill_alpha=0.9)\n",
    "\n",
    "\n",
    "graph_renderer.edge_renderer.glyph = MultiLine(line_color=\"#CCCCCC\", line_alpha=0.2, line_width=1)\n",
    "graph_renderer.edge_renderer.selection_glyph = MultiLine(line_color=Spectral4[2], line_width=1.5)\n",
    "graph_renderer.edge_renderer.hover_glyph = MultiLine(line_color=Spectral4[1], line_width=1.5)\n",
    "\n",
    "graph_renderer.selection_policy = NodesAndLinkedEdges()\n",
    "\n",
    "node_hover_tool = HoverTool(tooltips=[(\"Topic\", \"@topic_name\")])\n",
    "plot.add_tools(node_hover_tool, TapTool())\n",
    "\n",
    "plot.renderers.append(graph_renderer)\n",
    "\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply to CORDIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cordis_projects_df = pd.read_csv(os.path.join(inter_data_path, 'fp7_h2020_projects.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cordis_abstracts = [remove_markup(a) for a in cordis_projects_df['objective'][:25]]\n",
    "cordis_abstracts = [normalise_digits(a) for a in cordis_abstracts]\n",
    "cordis_abstracts = lemmatize(cordis_abstracts, nlp)\n",
    "cordis_abstracts = bigram(cordis_abstracts, phraser=bigrammer)\n",
    "cordis_abstracts = list(stringify_docs(cordis_abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for abstract, pred in zip(cordis_projects_df['objective'][:25], pipe.predict(cordis_abstracts)):\n",
    "    print(pred)\n",
    "    print(abstract)\n",
    "    print('\\n==============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cordis_tfidf_vecs = tfidf.transform(cordis_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cordis_subject_probs = rf_random.best_estimator_.predict_proba(cordis_tfidf_vecs)\n",
    "cordis_subjects = rf_random.best_estimator_.predict(cordis_tfidf_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_probs = np.zeros((len(cordis_projects_df), 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    subject_probs[:, i] = cordis_subject_probs[i][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cordis_projects_df['objective'][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cordis_subjects, columns=mlb.classes_).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_terms = []\n",
    "indices = np.array(range(0, X_train.shape[1]))\n",
    "for discipline in y_train.columns:\n",
    "    features_chi2 = chi2(X_train, y_train[discipline])[0]\n",
    "    threshold = np.percentile(features_chi2[~pd.isnull(features_chi2)], 90)\n",
    "    discipline_indices = indices[features_chi2 > threshold]\n",
    "    feature_terms.extend(np.array(tfidf.get_feature_names())[discipline_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_stop_words = set(tfidf.get_feature_names()).difference(set(feature_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "#     max_df=0.5, \n",
    "    min_df=5, \n",
    "    sublinear_tf=True, \n",
    "    norm='l2',\n",
    "    stop_words=tfidf_stop_words\n",
    ")\n",
    "tfidf_vecs_filt = tfidf.fit_transform(abstracts_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_vecs_filt, target_binarized, train_size=0.9, test_size=0.1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
