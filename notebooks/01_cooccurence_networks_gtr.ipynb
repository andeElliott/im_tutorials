{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GtR Topic Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing useful Python utility libraries\n",
    "import ast\n",
    "import re\n",
    "import smart_open\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain, combinations\n",
    "\n",
    "from im_tutorials.utilities import chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas is the defacto Python package for transforming and analysing\n",
    "# tabular data.\n",
    "# we will refer to pandas as pd to reduce the length of our code\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cols = ['research_topics', 'research_subjects']\n",
    "\n",
    "gtr_projects_df = pd.read_csv(\n",
    "    smart_open.smart_open('s3://innovation-mapping-tutorials/gateway-to-research/gtr_projects.csv'),\n",
    "    converters={k: ast.literal_eval for k in list_cols}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_projects_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the lists of research subjects and elements and count the contents\n",
    "research_subject_counter = Counter(chain(*gtr_projects_df['research_subjects']))\n",
    "research_topic_counter = Counter(chain(*gtr_projects_df['research_topics']))\n",
    "print('There are {} unique research subjects in the GtR projects dataset.'.format(len(research_subject_counter)))\n",
    "print('There are {} unique research topics in the GtR projects dataset.'.format(len(research_topic_counter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top Research Topics by Frequency\", '\\n')\n",
    "print('{:<40}{}'.format('Topic', 'Frequency'))\n",
    "for k, v in research_topic_counter.most_common(20):\n",
    "    print('{:<40}{}'.format(k, v))\n",
    "    \n",
    "print('\\nMedian Topic Freqency:')\n",
    "print(np.median(list(research_topic_counter.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the top research topic is _Climate & Climate Change_ by some margin. However, we can also see that the top spots are populated by topics from several disciplines. 50% of the topics occur 69 times or fewer, again highlighting the skewness of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field Definition Through Community Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to define communities of research topics as groups of topics which commonly occur together. An effective way of finding these clusters, and visualising the results, is by creating a topic cooccurrence graph.\n",
    "\n",
    "A cooccurrence graph is a network structure, where nodes are elements and an edge represents the elements of two nodes having cooccured at least once. The edges can then be \"weighted\" by the frequencies of each cooccurring pair. In the case of our research projects, we can say that two topics have cooccurred if they appear in at least one project together. To find all cooccurrences we therefore need to find the pairwise combinations of research topics for every project. For example, a single project with the topics\n",
    "```\n",
    "['Materials Characterisation', 'High Performance Computing', 'Condensed Matter Physics']\n",
    "```\n",
    "\n",
    "will become a set of topic pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The combinations function from itertools generates all the possible\n",
    "# elements of combinations from a list with length  r.\n",
    "list(combinations(['Materials Characterisation', 'High Performance Computing', 'Condensed Matter Physics'], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These cooccurrences would form a triangular network, where each edge has a frequency weight of 1.\n",
    "\n",
    "To create a cooccurrence network across all projects, we need to repeat this process for every project. We can do this in a Python list comprehension, and then chain togeher all of the cooccurring pairs into one long list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate every pairwise combination of research topics from each project.\n",
    "# Each pair is sorted alphabetically to make sure that there is only one \n",
    "# possible permutation of each edge.\n",
    "cooccurrences = list(chain(*[sorted(itertools.combinations(d, 2)) for d in gtr_projects_df['research_topics']]))\n",
    "# Count the frequency of each cooccurring pair.\n",
    "research_topic_edge_counter = Counter(cooccurrences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top Research Topic Cooccurrences by Frequency\", '\\n')\n",
    "print('{:<70}{}'.format('Cooccurrence', 'Frequency'))\n",
    "for k, v in research_topic_edge_counter.most_common(20):\n",
    "    print('{:<70}{}'.format((k[0] + ' + ' +k[1]), v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the most frequently cooccurring topics we can pairs that make intuitive sense and are all generally captured neatly within higher order academic disciplines.\n",
    "\n",
    "However this, along with the individual topic frequencies, also shows us that using the cooccurrence frequency as our edge weight might not be such a good idea. High frequency elements are simply more likely to cooccur due to chance. Therefore we should normalise our edge weights. One method for this is to calculate the association strength, a proababilistic measure, where the cooccurrence freqency is normalised by the product of the individual terms' occurrence counts. It is defined as\n",
    "\n",
    "$$ a = \\frac{2 n c_{ij}}{o_{i}o_{j}} $$\n",
    "\n",
    "where $n$ is the total number of elements, $c_{ij}$ is the number of cooccurrences between elements $i$ and $j$, and $o_{i}$ and $o_{j}$ are the individual frequency counts of each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def association_strength(combo, occurrences, cooccurrences, total):\n",
    "    '''association_strength\n",
    "    Calculates the association strength between a cooccurring pair.\n",
    "    '''\n",
    "    a_s = ((2 * total * cooccurrences[combo]) / \n",
    "           (occurrences[combo[0]] * occurrences[combo[1]]))\n",
    "    return a_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build our cooccurrence network, we need to generate a list of unique edges from our long list of cooccurrences and then calculate the association strength for each edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a set of cooccurences (a list of unique pairs).\n",
    "# This will form the edges of our cooccurrence graph.\n",
    "edges = set(cooccurrences)\n",
    "# Calculate the total number of elements\n",
    "n = len(list(chain(*gtr_projects_df['research_topics'])))\n",
    "# Calculate the association strength for each edge.\n",
    "# We take the log of the association strength to give it\n",
    "# a normal distribution.\n",
    "assoc_strengths = np.log10([association_strength(\n",
    "    edge,\n",
    "    research_topic_counter, \n",
    "    research_topic_edge_counter, \n",
    "    total_research_topics) for edge in edges])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(assoc_strengths, bins=100)\n",
    "ax.set_xlabel('Association Strength')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_edges = []\n",
    "for (s, t), a_s in zip(edges, assoc_strengths):\n",
    "    weighted_edges.append((s, t, a_s))\n",
    "\n",
    "\n",
    "g = nx.Graph()\n",
    "g.add_weighted_edges_from(weighted_edges, weight='association_strength')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g.edges[('Materials Characterisation','Materials Synthesis & Growth')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = community.best_partition(g, resolution=0.7, random_state=1, weight='weight')\n",
    "n_communities = len(set(part.values()))\n",
    "print('{} communities detected.'.format(n_communities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Network Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we add some extra properties to the nodes in our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {k: k for k, _ in part.items()}\n",
    "nx.set_node_attributes(g, names, name='topic_name')\n",
    "community_colors = {k: Category20[n_communities][c] for k, c in part.items()}\n",
    "nx.set_node_attributes(g, community_colors, name='color')\n",
    "\n",
    "print(g.nodes['Materials Characterisation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we calculate positions for the visual graph layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(g, weight='association_strength', scale=2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bokeh` has built-in support for `networkx` graphs, which makes plotting and interacting with them easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = figure(title=\"Research Topic Cooccurrence Network\",\n",
    "              x_range=(-2.1,2.1), y_range=(-2.1,2.1),\n",
    "             )\n",
    "\n",
    "graph_renderer = from_networkx(g, pos, center=(0,0))\n",
    "graph_renderer.node_renderer.glyph = Circle(size=7, fill_color='color', line_color=None)\n",
    "graph_renderer.node_renderer.selection_glyph = Circle(size=7, fill_color='color')\n",
    "graph_renderer.node_renderer.hover_glyph = Circle(size=7, fill_color='color')\n",
    "graph_renderer.node_renderer.muted_glyph = Circle(size=7, fill_color='color', fill_alpha=0.9)\n",
    "\n",
    "\n",
    "graph_renderer.edge_renderer.glyph = MultiLine(line_color=\"#CCCCCC\", line_alpha=0.2, line_width=1)\n",
    "graph_renderer.edge_renderer.selection_glyph = MultiLine(line_color=Spectral4[2], line_width=1.5)\n",
    "graph_renderer.edge_renderer.hover_glyph = MultiLine(line_color=Spectral4[1], line_width=1.5)\n",
    "\n",
    "graph_renderer.selection_policy = NodesAndLinkedEdges()\n",
    "\n",
    "node_hover_tool = HoverTool(tooltips=[(\"Topic\", \"@topic_name\")])\n",
    "plot.add_tools(node_hover_tool, TapTool())\n",
    "\n",
    "plot.renderers.append(graph_renderer)\n",
    "\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the Communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's manually inspect the topics in each community to see if we can see what disciplines they might form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_part = defaultdict(list)\n",
    "for k, v in part.items():\n",
    "    reverse_part[v].append(k)\n",
    "    \n",
    "for c, topics in reverse_part.items():\n",
    "    print(c)\n",
    "    for chunk in chunks(topics, 4):\n",
    "        print(', '.join(chunk))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a community ID to discipline mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discipline_map = {\n",
    "    0: ,\n",
    "    1: ,\n",
    "    2: ,\n",
    "    3: ,\n",
    "    4: ,\n",
    "    5: ,\n",
    "    6: ,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommunityPartition:\n",
    "    def __init__(self, graph):\n",
    "        self.graph = graph\n",
    "    \n",
    "    def edgelist_to_cooccurrence(self, repeats, **best_partition_kwargs):\n",
    "        edge_counter = Counter()\n",
    "        for i in range(repeats):\n",
    "            partition = community.best_partition(self.graph, random_state=i, **best_partition_kwargs)\n",
    "            edgelist = self.partition_to_edgelist(partition)\n",
    "            edge_counter.update(edgelist)\n",
    "\n",
    "        g = nx.Graph()\n",
    "        g.add_weighted_edges_from([(e[0][0], e[0][1], e[1]) for e in edge_counter.items()])\n",
    "        return g\n",
    "    \n",
    "    def partition_to_edgelist(self, partition):\n",
    "        partition_reverse_mapping = self.reverse_index_partition(partition)\n",
    "        edgelist = []\n",
    "        for community, elements in partition_reverse_mapping.items():\n",
    "            combos = [tuple(sorted(e)) for e in itertools.combinations(elements, 2)]\n",
    "            edgelist.extend(combos)\n",
    "        return edgelist\n",
    "     \n",
    "    def reverse_index_partition(self, partition):\n",
    "        partition_reverse_mapping = defaultdict(list)\n",
    "        for k, v in partition.items():\n",
    "            partition_reverse_mapping[v].append(k)\n",
    "        return partition_reverse_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = CommunityPartition(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = cp.edgelist_to_cooccurrence(3, resolution=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the best partition\n",
    "part = community.best_partition(g, resolution=0.8, random_state=0, weight='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_partition = defaultdict(list)\n",
    "for k, v in part.items():\n",
    "    reverse_partition[v].append(k)\n",
    "    \n",
    "names = {k: dictionary[k] for k, _ in part.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {k: dictionary[k] for k, _ in part.items()}\n",
    "nx.set_node_attributes(co, part, name='community')\n",
    "nx.set_node_attributes(co, names, name='topic')\n",
    "community_colors = {k: Category20[20][c] for k, c in part.items()}\n",
    "nx.set_node_attributes(co, community_colors, name='color')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(part.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(co, weight='weight', scale=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_name_lookup = {\n",
    "    0: 'social_sciences',\n",
    "    1: 'arts_linguistics',\n",
    "    2: 'chem_mater_phys_eng',\n",
    "    3: 'social_sciences',\n",
    "    4: 'maths_computing_ee',\n",
    "    5: 'social_sciences',\n",
    "    6: 'social_sciences',\n",
    "    7: 'biological_sciences',\n",
    "    8: 'social_sciences',\n",
    "    9: 'humanities',\n",
    "    10: 'arts_linguistics',\n",
    "    11: 'humanities',\n",
    "    12: 'social_sciences',\n",
    "    13: 'physics',\n",
    "    14: 'environmental_sciences',\n",
    "    15: 'social_sciences',\n",
    "    16: 'humanities'\n",
    "}\n",
    "\n",
    "topic_discipline_lookup = {top:category_name_lookup[disc] for top, disc in part.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(model_path, 'communities_partition.pkl'), 'wb') as f:\n",
    "    pickle.dump(part, f)\n",
    "\n",
    "with open(os.path.join(model_path, 'communities_partition_labels.pkl'), 'wb') as f:\n",
    "    pickle.dump(category_name_lookup, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_projects_df['discipline'] = gtr_projects_df['research_topics'].apply(\n",
    "    lambda x: [topic_discipline_lookup[val] for val in x])\n",
    "\n",
    "gtr_projects_df['discipline_sets'] = [set(x) for x in gtr_projects_df['discipline']]\n",
    "\n",
    "gtr_projects_df['single_disc'] = [True if len(x)==1 else np.nan if len(x)==0 else False for x in gtr_projects_df['discipline_sets']]\n",
    "\n",
    "gtr_projects_df['single_disc'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_projects_df['discipline_sets'] = [\n",
    "    set(['medical_sciences']) if f =='MRC' else x for f,x in zip(\n",
    "        gtr_projects_df['funder_name'],\n",
    "           gtr_projects_df['discipline_sets'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modal_value(l):\n",
    "    c = Counter(l)\n",
    "    try:\n",
    "        return c.most_common(1)[0][0]\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "gtr_projects_df['modal_discipline'] = [modal_value(d) for d in gtr_projects_df['discipline_sets']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_projects_df['modal_discipline'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(chain(*gtr_projects_df['discipline_sets'])).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = [True if len(s) > 0 else False for s in gtr_projects_df['discipline_sets']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove projects without abstracts\n",
    "gtr_projects_df = gtr_projects_df[~pd.isnull(gtr_projects_df['abstract_texts'])]\n",
    "# remove projects with short abstracts\n",
    "gtr_projects_df = gtr_projects_df[gtr_projects_df['abstract_texts'].str.len() > 250]\n",
    "# remove projects with no labels\n",
    "n_labels = [True if len(s) > 0 else False for s in gtr_projects_df['discipline_sets']]\n",
    "gtr_projects_df = gtr_projects_df[n_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from gensim.models.phrases import Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "nlp.remove_pipe('parser')\n",
    "nlp.remove_pipe('ner')\n",
    "\n",
    "with open(os.path.join(raw_data_path, 'stopwords_en_long.txt'), 'r') as f:\n",
    "    stopwords = f.read().splitlines()\n",
    "\n",
    "for stopword in stopwords:\n",
    "    nlp.vocab[stopword.lower()].is_stop = True\n",
    "    nlp.vocab[stopword.upper()].is_stop = True\n",
    "    nlp.vocab[stopword.title()].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = [remove_markup(a) for a in gtr_projects_df['abstract_texts']]\n",
    "abstracts = [normalise_digits(a) for a in abstracts]\n",
    "abstracts = lemmatize(abstracts, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrammer = Phraser.load(os.path.join(model_path, 'gtr_discipline_bigrammer.pkl'))\n",
    "abstracts = bigram(abstracts, phraser=bigrammer)\n",
    "abstracts_str = list(stringify_docs(abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_projects_df['abstract_processed'] = abstracts_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import ComplementNB, MultinomialNB\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_projects_df['n_disciplines'] = [len(a) for a in gtr_projects_df['discipline_sets']]\n",
    "gtr_pure_df = gtr_projects_df[gtr_projects_df['n_disciplines'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_single = TfidfVectorizer(\n",
    "    max_df=0.4, \n",
    "    min_df=5,\n",
    "    sublinear_tf=True, \n",
    "    norm='l2'\n",
    ")\n",
    "tfidf_pure_vecs = tfidf_single.fit_transform(gtr_pure_df['abstract_processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_pure_vecs, gtr_pure_df['modal_discipline'], train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = SelectPercentile(chi2, 40)\n",
    "sp_vecs = sp.fit_transform(X_train, y_train)\n",
    "sp_vecs_test = sp.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=10, solver='lbfgs', multi_class='auto')\n",
    "mnb = MultinomialNB()\n",
    "cmb = ComplementNB()\n",
    "voter = VotingClassifier(estimators=[('lr', lr), ('cmb', cmb), ('mnb', mnb)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params = {'C': [0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100]}\n",
    "lr_grid = GridSearchCV(lr, param_grid=lr_params, cv=5, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_grid.fit(sp_vecs, y_train)\n",
    "lr_best = lr_grid.best_estimator_\n",
    "print(classification_report(y_test, lr_best.predict(sp_vecs_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(\n",
    "    pd.DataFrame(confusion_matrix(y_test, lr_best.predict(sp_vecs_test)), columns=mlb.classes_, index=mlb.classes_),\n",
    "    annot=True,\n",
    "    fmt='d'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_single = TfidfVectorizer(\n",
    "    max_df=0.4, \n",
    "    min_df=5,\n",
    "    sublinear_tf=True, \n",
    "    norm='l2'\n",
    ")\n",
    "sp = SelectPercentile(chi2, 40)\n",
    "lr = LogisticRegression(C=10, solver='lbfgs', multi_class='auto')\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        ('tfidf', tfidf),\n",
    "        ('sp', sp),\n",
    "        ('lr', lr)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(gtr_pure_df['abstract_processed'], gtr_pure_df['modal_discipline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(pipe, os.path.join(model_path, 'gtr_discipline_lvl9_lr_20190222.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    max_df=0.3, \n",
    "    min_df=5,\n",
    "    sublinear_tf=True, \n",
    "    norm='l2'\n",
    ")\n",
    "tfidf_vecs = tfidf.fit_transform(abstracts_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(set(chain(*gtr_projects_df['discipline_sets'])))\n",
    "mlb = MultiLabelBinarizer(classes=classes)\n",
    "target_binarized = mlb.fit_transform(gtr_projects_df['discipline_sets'])\n",
    "target_binarized_df = pd.DataFrame(target_binarized, columns=mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_vecs, target_binarized_df, train_size=0.8, test_size=0.2)\n",
    "sp = SelectPercentile(chi2, 40)\n",
    "sp_vecs = sp.fit_transform(X_train, y_train)\n",
    "sp_vecs_test = sp.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='lbfgs', multi_class='auto')\n",
    "mnb = MultinomialNB()\n",
    "cmb = ComplementNB()\n",
    "voter = VotingClassifier(estimators=[('lr', lr), ('cmb', cmb), ('mnb', mnb)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cls in mlb.classes_:\n",
    "    print(cls)\n",
    "    lr.fit(sp_vecs, y_train[cls])\n",
    "    print(classification_report(y_test[cls], lr.predict(sp_vecs_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, voter.predict(sp_vecs_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "     'bootstrap': [True, False],\n",
    "     'max_depth': [10, 20, 50, 100, None],\n",
    "     'max_features': ['auto', 'sqrt'],\n",
    "     'min_samples_leaf': [1, 2, 4],\n",
    "     'min_samples_split': [2, 5, 10],\n",
    "     'n_estimators': [100, 200, 500]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_jobs=3)\n",
    "\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=rf_params,\n",
    "    n_iter=100,\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, rf_random.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_estimator_.fit(tfidf_vecs_filt, target_binarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(rf_random.best_estimator_, os.path.join(model_path, 'gtr_discipline_classifier_rf_8_20192102.pkl'))\n",
    "joblib.dump(tfidf, os.path.join(model_path, 'gtr_discipline_tfidf_20192102.pkl'))\n",
    "bigrammer.save(os.path.join(model_path, 'gtr_discipline_bigrammer.pkl'))\n",
    "nlp.vocab.to_disk(os.path.join(model_path, 'gtr_discipline_vocab'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply to CORDIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cordis_projects_df = pd.read_csv(os.path.join(inter_data_path, 'fp7_h2020_projects.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cordis_abstracts = [remove_markup(a) for a in cordis_projects_df['objective'][:25]]\n",
    "cordis_abstracts = [normalise_digits(a) for a in cordis_abstracts]\n",
    "cordis_abstracts = lemmatize(cordis_abstracts, nlp)\n",
    "cordis_abstracts = bigram(cordis_abstracts, phraser=bigrammer)\n",
    "cordis_abstracts = list(stringify_docs(cordis_abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for abstract, pred in zip(cordis_projects_df['objective'][:25], pipe.predict(cordis_abstracts)):\n",
    "    print(pred)\n",
    "    print(abstract)\n",
    "    print('\\n==============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cordis_tfidf_vecs = tfidf.transform(cordis_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cordis_subject_probs = rf_random.best_estimator_.predict_proba(cordis_tfidf_vecs)\n",
    "cordis_subjects = rf_random.best_estimator_.predict(cordis_tfidf_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_probs = np.zeros((len(cordis_projects_df), 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    subject_probs[:, i] = cordis_subject_probs[i][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cordis_projects_df['objective'][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cordis_subjects, columns=mlb.classes_).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_terms = []\n",
    "indices = np.array(range(0, X_train.shape[1]))\n",
    "for discipline in y_train.columns:\n",
    "    features_chi2 = chi2(X_train, y_train[discipline])[0]\n",
    "    threshold = np.percentile(features_chi2[~pd.isnull(features_chi2)], 90)\n",
    "    discipline_indices = indices[features_chi2 > threshold]\n",
    "    feature_terms.extend(np.array(tfidf.get_feature_names())[discipline_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_stop_words = set(tfidf.get_feature_names()).difference(set(feature_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "#     max_df=0.5, \n",
    "    min_df=5, \n",
    "    sublinear_tf=True, \n",
    "    norm='l2',\n",
    "    stop_words=tfidf_stop_words\n",
    ")\n",
    "tfidf_vecs_filt = tfidf.fit_transform(abstracts_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_vecs_filt, target_binarized, train_size=0.9, test_size=0.1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
